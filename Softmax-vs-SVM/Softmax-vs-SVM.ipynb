{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "@misc{hgupta01,\n",
        "  author = {hgupta01},\n",
        "  title = {svm_classification_keras},\n",
        "  year = {2019},\n",
        "  publisher = {GitHub},\n",
        "  journal = {GitHub repository},\n",
        "  url = {\\url{https://github.com/hgupta01/svm_classification_keras}}\n",
        "}"
      ],
      "metadata": {
        "id": "nBHViyqlL-J9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VZTdYWrqwO_D"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input, Activation, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "import numpy as np\n",
        "\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Input\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten\n",
        "from keras import backend as k\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BEST MODELS"
      ],
      "metadata": {
        "id": "zwRhWza8ql2S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAFGA5pbwVAW",
        "outputId": "dc379eaa-9840-40d4-a221-666ab97d7301"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "60000 train samples\n",
            "10000 test samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "938/938 [==============================] - 34s 35ms/step - loss: 9.9445 - accuracy: 0.8264 - val_loss: 9.8505 - val_accuracy: 0.9312\n",
            "Epoch 2/4\n",
            "938/938 [==============================] - 14s 15ms/step - loss: 9.8603 - accuracy: 0.9247 - val_loss: 9.8316 - val_accuracy: 0.9493\n",
            "Epoch 3/4\n",
            "938/938 [==============================] - 13s 14ms/step - loss: 9.8431 - accuracy: 0.9416 - val_loss: 9.8222 - val_accuracy: 0.9597\n",
            "Epoch 4/4\n",
            "938/938 [==============================] - 13s 14ms/step - loss: 9.8315 - accuracy: 0.9528 - val_loss: 9.8171 - val_accuracy: 0.9635\n",
            "Test loss: 9.817072868347168\n",
            "Test accuracy: 0.9635000228881836\n"
          ]
        }
      ],
      "source": [
        "#MNIST (SVM)\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255.\n",
        "x_test /= 255.\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "def model_1(x_input):\n",
        "    x = Dense(512, activation='relu')(x_input)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x_out = Dense(256, activation='relu')(x)\n",
        "    x_out = Dropout(0.5)(x)\n",
        "    return x_out\n",
        "  \n",
        "def model_2(x_input):\n",
        "    x = Dense(800, activation='sigmoid')(x_input)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(200, activation='sigmoid')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x_out = Dense(12)(x)\n",
        "    return x_out\n",
        "\n",
        "inputs = Input(shape=(784,))\n",
        "x      = model_1(inputs)\n",
        "x_out  = Dense(10, use_bias=False, activation='linear', name='svm')(x)\n",
        "model = Model(inputs, x_out)\n",
        "\n",
        "def svm_loss(layer):\n",
        "    weights = layer.weights[0]\n",
        "    weights_tf = tf.convert_to_tensor(weights)\n",
        "    \n",
        "    def categorical_hinge_loss(y_true, y_pred):\n",
        "        pos = K.sum(y_true * y_pred, axis=-1)\n",
        "        neg = K.max((1.0 - y_true) * y_pred, axis=-1)\n",
        "        hinge_loss = K.mean(K.maximum(0.0, neg - pos + 1), axis=-1)\n",
        "        regularization_loss = 0.5*(tf.reduce_sum(tf.square(weights_tf)))\n",
        "        return regularization_loss + 0.4*hinge_loss\n",
        "    \n",
        "    return categorical_hinge_loss\n",
        "\n",
        "metrics = ['accuracy']\n",
        "optimizer = tf.keras.optimizers.SGD(lr=0.1, momentum=0.9)\n",
        "#optimizer = tf.keras.optimizers.legacy.RMSprop(lr=2e-3, decay=1e-5)\n",
        "#optimizer = tf.train.AdamOptimizer(1.e-3)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=svm_loss(model.get_layer('svm')), metrics=metrics)\n",
        "\n",
        "batch_size = 200\n",
        "epochs = 4\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bc8cd7_XVCy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "778e02e3-36ea-49da-bc9d-3b6d559ae0d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000 train samples\n",
            "10000 test samples\n",
            "Epoch 1/4\n",
            "938/938 [==============================] - 13s 14ms/step - loss: 2.3278 - accuracy: 0.1414 - val_loss: 2.1631 - val_accuracy: 0.3480\n",
            "Epoch 2/4\n",
            "938/938 [==============================] - 13s 14ms/step - loss: 2.1761 - accuracy: 0.2210 - val_loss: 2.0268 - val_accuracy: 0.5438\n",
            "Epoch 3/4\n",
            "938/938 [==============================] - 13s 14ms/step - loss: 2.0495 - accuracy: 0.3092 - val_loss: 1.8994 - val_accuracy: 0.6307\n",
            "Epoch 4/4\n",
            "938/938 [==============================] - 13s 14ms/step - loss: 1.9302 - accuracy: 0.3878 - val_loss: 1.7755 - val_accuracy: 0.6827\n",
            "Test loss: 1.775486946105957\n",
            "Test accuracy: 0.682699978351593\n"
          ]
        }
      ],
      "source": [
        "#MNIST (softmax)\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255.\n",
        "x_test /= 255.\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "def model_1(x_input):\n",
        "    x = Dense(512, activation='relu')(x_input)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x_out = Dense(256, activation='relu')(x)\n",
        "    x_out = Dropout(0.5)(x)\n",
        "    return x_out\n",
        "  \n",
        "def model_2(x_input):\n",
        "    x = Dense(800, activation='sigmoid')(x_input)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(200, activation='sigmoid')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x_out = Dense(12)(x)\n",
        "    return x_out\n",
        "\n",
        "inputs = Input(shape=(784,))\n",
        "x      = model_1(inputs)\n",
        "x_out  = Dense(10, use_bias=False, activation='softmax')(x)\n",
        "model = Model(inputs, x_out)\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.Adadelta(),\n",
        "              loss=keras.losses.categorical_crossentropy,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#metrics = ['accuracy']\n",
        "#optimizer = tf.keras.optimizers.legacy.RMSprop(lr=2e-3, decay=1e-5)\n",
        "#optimizer = tf.train.AdamOptimizer(1.e-3)\n",
        "\n",
        "#model.compile(optimizer=optimizer, loss=svm_loss(model.get_layer('svm')), metrics=metrics)\n",
        "\n",
        "batch_size = 200\n",
        "epochs = 4\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FS-jhu5JYm6R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84bdd827-1760-4c27-f18b-9b852daacbe8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 7s 0us/step\n",
            "50000 train samples\n",
            "10000 test samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/optimizers/legacy/rmsprop.py:143: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "391/391 [==============================] - 32s 75ms/step - loss: 10.2673 - accuracy: 0.1069 - val_loss: 10.2357 - val_accuracy: 0.1002\n",
            "Epoch 2/4\n",
            "391/391 [==============================] - 22s 57ms/step - loss: 10.2357 - accuracy: 0.1000 - val_loss: 10.2357 - val_accuracy: 0.1005\n",
            "Epoch 3/4\n",
            "391/391 [==============================] - 21s 54ms/step - loss: 10.2357 - accuracy: 0.1000 - val_loss: 10.2357 - val_accuracy: 0.1002\n",
            "Epoch 4/4\n",
            "391/391 [==============================] - 23s 58ms/step - loss: 10.2357 - accuracy: 0.1001 - val_loss: 10.2357 - val_accuracy: 0.1003\n",
            "Test loss: 10.235673904418945\n",
            "Test accuracy: 0.10029999911785126\n"
          ]
        }
      ],
      "source": [
        "#CIFAR10 (SVM)\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "x_train = x_train.reshape(50000, 32*32*3)\n",
        "x_test = x_test.reshape(10000, 32*32*3)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255.\n",
        "x_test /= 255.\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "def model_1(x_input):\n",
        "    x = Dense(512, activation='relu')(x_input)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x_out = Dense(256, activation='relu')(x)\n",
        "    x_out = Dropout(0.2)(x)\n",
        "    return x_out\n",
        "  \n",
        "def model_2(x_input):\n",
        "    x = Dense(800, activation='sigmoid')(x_input)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Dense(200, activation='sigmoid')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x_out = Dense(12)(x)\n",
        "    return x_out\n",
        "\n",
        "inputs = Input(shape=(32*32*3,))\n",
        "x      = model_1(inputs)\n",
        "x_out  = Dense(10, use_bias=False, activation='linear', name='svm')(x)\n",
        "model = Model(inputs, x_out)\n",
        "\n",
        "def svm_loss(layer):\n",
        "    weights = layer.weights[0]\n",
        "    weights_tf = tf.convert_to_tensor(weights)\n",
        "    \n",
        "    def categorical_hinge_loss(y_true, y_pred):\n",
        "        pos = K.sum(y_true * y_pred, axis=-1)\n",
        "        neg = K.max((1.0 - y_true) * y_pred, axis=-1)\n",
        "        hinge_loss = K.mean(K.maximum(0.0, neg - pos + 1), axis=-1)\n",
        "        regularization_loss = 0.5*(tf.reduce_sum(tf.square(weights_tf)))\n",
        "        return regularization_loss + 0.4*hinge_loss\n",
        "    \n",
        "    return categorical_hinge_loss\n",
        "\n",
        "metrics = ['accuracy']\n",
        "optimizer = tf.keras.optimizers.legacy.RMSprop(lr=2e-3, decay=1e-5)\n",
        "#optimizer = tf.train.AdamOptimizer(1.e-3)\n",
        "\n",
        "model.compile(optimizer='adam', loss=svm_loss(model.get_layer('svm')), metrics=metrics)\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 4\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87WE_zT1f9a-",
        "outputId": "ebbe31db-5dc1-46b8-f3a1-c39d8e240384"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50000 train samples\n",
            "10000 test samples\n",
            "Epoch 1/4\n",
            "391/391 [==============================] - 23s 56ms/step - loss: 1.9238 - accuracy: 0.3080 - val_loss: 1.7072 - val_accuracy: 0.3943\n",
            "Epoch 2/4\n",
            "391/391 [==============================] - 23s 59ms/step - loss: 1.7054 - accuracy: 0.3881 - val_loss: 1.6188 - val_accuracy: 0.4225\n",
            "Epoch 3/4\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 1.6218 - accuracy: 0.4224 - val_loss: 1.5931 - val_accuracy: 0.4279\n",
            "Epoch 4/4\n",
            "391/391 [==============================] - 23s 58ms/step - loss: 1.5676 - accuracy: 0.4400 - val_loss: 1.5093 - val_accuracy: 0.4585\n",
            "Test loss: 1.5093368291854858\n",
            "Test accuracy: 0.4584999978542328\n"
          ]
        }
      ],
      "source": [
        "#CIFAR10 (Softmax)\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "x_train = x_train.reshape(50000, 32*32*3)\n",
        "x_test = x_test.reshape(10000, 32*32*3)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255.\n",
        "x_test /= 255.\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "def model_1(x_input):\n",
        "    x = Dense(512, activation='relu')(x_input)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x_out = Dense(256, activation='relu')(x)\n",
        "    x_out = Dropout(0.2)(x)\n",
        "    return x_out\n",
        "  \n",
        "def model_2(x_input):\n",
        "    x = Dense(800, activation='sigmoid')(x_input)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Dense(200, activation='sigmoid')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x_out = Dense(12)(x)\n",
        "    return x_out\n",
        "\n",
        "inputs = Input(shape=(32*32*3,))\n",
        "x      = model_1(inputs)\n",
        "x_out  = Dense(10, use_bias=False, activation='softmax')(x)\n",
        "model = Model(inputs, x_out)\n",
        "\n",
        "model.compile(optimizer='adam', loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
        "#optimizer = tf.train.AdamOptimizer(1.e-3)\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 4\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PAPER MODELS (Replication to the best of our abilities)"
      ],
      "metadata": {
        "id": "vW6HbwD1qsg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#MNIST (SVM) - According to paper\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255.\n",
        "x_test /= 255.\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "def model_1(x_input):\n",
        "    x = Dense(512, activation='relu')(x_input)\n",
        "    x_out = Dense(512, activation='relu')(x)\n",
        "\n",
        "    return x_out\n",
        "  \n",
        "def model_2(x_input):\n",
        "    x = Dense(800, activation='sigmoid')(x_input)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(200, activation='sigmoid')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x_out = Dense(12)(x)\n",
        "    return x_out\n",
        "\n",
        "inputs = Input(shape=(784,))\n",
        "x      = model_1(inputs)\n",
        "x_out  = Dense(10, use_bias=False, activation='linear', name='svm')(x)\n",
        "model = Model(inputs, x_out)\n",
        "\n",
        "def svm_loss(layer):\n",
        "    weights = layer.weights[0]\n",
        "    weights_tf = tf.convert_to_tensor(weights)\n",
        "    \n",
        "    def categorical_hinge_loss(y_true, y_pred):\n",
        "        pos = K.sum(y_true * y_pred, axis=-1)\n",
        "        neg = K.max((1.0 - y_true) * y_pred, axis=-1)\n",
        "        hinge_loss = K.mean(K.maximum(0.0, neg - pos + 1), axis=-1)\n",
        "        regularization_loss = 0.5*(tf.reduce_sum(tf.square(weights_tf)))\n",
        "        return regularization_loss + 0.4*hinge_loss\n",
        "    \n",
        "    return categorical_hinge_loss\n",
        "\n",
        "metrics = ['accuracy']\n",
        "optimizer = tf.keras.optimizers.SGD(lr=0.1, momentum=0.9)\n",
        "model.compile(optimizer=optimizer, loss=svm_loss(model.get_layer('svm')), metrics=metrics)\n",
        "\n",
        "batch_size = 200\n",
        "epochs = 4\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n"
      ],
      "metadata": {
        "id": "okO1SPJGQkPk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbdf50c1-6700-4b29-8f12-3b5e8afe512d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000 train samples\n",
            "10000 test samples\n",
            "Epoch 1/4\n",
            "300/300 [==============================] - 6s 20ms/step - loss: 9.8811 - accuracy: 0.8403 - val_loss: 9.8052 - val_accuracy: 0.9253\n",
            "Epoch 2/4\n",
            "300/300 [==============================] - 5s 15ms/step - loss: 9.7979 - accuracy: 0.9309 - val_loss: 9.7860 - val_accuracy: 0.9422\n",
            "Epoch 3/4\n",
            "300/300 [==============================] - 6s 19ms/step - loss: 9.7824 - accuracy: 0.9446 - val_loss: 9.7767 - val_accuracy: 0.9495\n",
            "Epoch 4/4\n",
            "300/300 [==============================] - 4s 15ms/step - loss: 9.7731 - accuracy: 0.9532 - val_loss: 9.7697 - val_accuracy: 0.9568\n",
            "Test loss: 9.769701957702637\n",
            "Test accuracy: 0.9567999839782715\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MNIST (softmax) - According to paper\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255.\n",
        "x_test /= 255.\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "def model_1(x_input):\n",
        "    x = Dense(512, activation='relu')(x_input)\n",
        "    x_out = Dense(512, activation='relu')(x)\n",
        "\n",
        "    return x_out\n",
        "  \n",
        "def model_2(x_input):\n",
        "    x = Dense(800, activation='sigmoid')(x_input)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(200, activation='sigmoid')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x_out = Dense(12)(x)\n",
        "    return x_out\n",
        "\n",
        "inputs = Input(shape=(784,))\n",
        "x      = model_1(inputs)\n",
        "x_out  = Dense(10, use_bias=False, activation='softmax')(x)\n",
        "model = Model(inputs, x_out)\n",
        "optimizer = tf.keras.optimizers.SGD(lr=0.1, momentum=0.9)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=keras.losses.categorical_crossentropy,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "batch_size = 200\n",
        "epochs = 4\n",
        "\n",
        "print(batch_size)\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n"
      ],
      "metadata": {
        "id": "fWsQFe0UQ0Kv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9f1e9b1-d50c-4253-bdab-ccee89147a06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000 train samples\n",
            "10000 test samples\n",
            "200\n",
            "Epoch 1/4\n",
            "300/300 [==============================] - 5s 15ms/step - loss: 0.5225 - accuracy: 0.8585 - val_loss: 0.2523 - val_accuracy: 0.9282\n",
            "Epoch 2/4\n",
            "300/300 [==============================] - 6s 20ms/step - loss: 0.2320 - accuracy: 0.9334 - val_loss: 0.1916 - val_accuracy: 0.9454\n",
            "Epoch 3/4\n",
            "300/300 [==============================] - 4s 15ms/step - loss: 0.1813 - accuracy: 0.9481 - val_loss: 0.1591 - val_accuracy: 0.9529\n",
            "Epoch 4/4\n",
            "300/300 [==============================] - 5s 18ms/step - loss: 0.1463 - accuracy: 0.9583 - val_loss: 0.1351 - val_accuracy: 0.9607\n",
            "Test loss: 0.1351233720779419\n",
            "Test accuracy: 0.9606999754905701\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CIFAR10 (SVM) - according to paper\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "x_train = x_train.reshape(50000, 32*32*3)\n",
        "x_test = x_test.reshape(10000, 32*32*3)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255.\n",
        "x_test /= 255.\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "x_train = x_train.reshape(50000, 32, 32, 3)\n",
        "x_test = x_test.reshape(10000, 32, 32, 3)\n",
        "\n",
        "'''def model_1(x_input):\n",
        "    x = Dense(512, activation='relu')(x_input)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x_out = Dense(256, activation='relu')(x)\n",
        "    x_out = Dropout(0.2)(x)\n",
        "    return x_out\n",
        "    '''\n",
        "\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten\n",
        "\n",
        "def model_1(x_input):\n",
        "    x = Conv2D(32, (5, 5), activation='relu',input_shape=(32,32,3))(x_input)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = Conv2D(64, (5, 5), activation='relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(3072, activation='relu')(x)\n",
        "    x_out = Dropout(0.2)(x)\n",
        "\n",
        "    return x_out\n",
        "\n",
        "\n",
        "inputs = Input(shape=(32, 32, 3))\n",
        "\n",
        "x      = model_1(inputs)\n",
        "x_out  = Dense(10, use_bias=False, activation='linear', name='svm')(x)\n",
        "model = Model(inputs, x_out)\n",
        "\n",
        "def svm_loss(layer):\n",
        "    weights = layer.weights[0]\n",
        "    weights_tf = tf.convert_to_tensor(weights)\n",
        "    \n",
        "    def categorical_hinge_loss(y_true, y_pred):\n",
        "        pos = K.sum(y_true * y_pred, axis=-1)\n",
        "        neg = K.max((1.0 - y_true) * y_pred, axis=-1)\n",
        "        hinge_loss = K.mean(K.maximum(0.0, neg - pos + 1), axis=-1)\n",
        "        regularization_loss = 0.5*(tf.reduce_sum(tf.square(weights_tf)))\n",
        "        return regularization_loss + 0.4*hinge_loss\n",
        "    \n",
        "    return categorical_hinge_loss\n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.AdamW()\n",
        "#optimizer = tf.train.AdamOptimizer(1.e-3)\n",
        "\n",
        "model.compile(optimizer='adam', loss=svm_loss(model.get_layer('svm')), metrics = ['accuracy'])\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 4\n",
        "\n"
      ],
      "metadata": {
        "id": "Xp4b0qkORC0V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c174d286-6e33-4da9-fd12-731d8441d79e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50000 train samples\n",
            "10000 test samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "4b8XRGW5iKmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "wgwoxf4Uh1oT",
        "outputId": "52f82d23-81ed-4d94-9825-edde019b2717"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            " 2/50 [>.............................] - ETA: 8:56 - loss: 10.9087 - accuracy: 0.1125 "
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-a4b8a2636f18>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model.fit(x_train, y_train,\n\u001b[0m\u001b[1;32m      2\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     validation_data=(x_test, y_test))\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CIFAR10 (Softmax) - according to paper\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "x_train = x_train.reshape(50000, 32, 32, 3)\n",
        "x_test = x_test.reshape(10000, 32, 32, 3)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255.\n",
        "x_test /= 255.\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "def model_1(x_input):\n",
        "    x = Conv2D(32, (5, 5), activation='relu')(x_input)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = Conv2D(64, (5, 5), activation='relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(3072, activation='relu')(x)\n",
        "    x_out = Dropout(0.2)(x)\n",
        "\n",
        "    return x_out\n",
        "\n",
        "\n",
        "inputs = Input(shape=(32, 32, 3))\n",
        "x      = model_1(inputs)\n",
        "x_out  = Dense(10, use_bias=False, activation='softmax')(x)\n",
        "model = Model(inputs, x_out)\n",
        "\n",
        "model.compile(optimizer='adam', loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
        "#optimizer = tf.train.AdamOptimizer(1.e-3)\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 4\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n"
      ],
      "metadata": {
        "id": "mhk661QURTYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EXPERIMENTS"
      ],
      "metadata": {
        "id": "rXrRQKaLq1V3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#best batch size mnist/svm\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255.\n",
        "x_test /= 255.\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "def model_1(x_input):\n",
        "    x = Dense(512, activation='relu')(x_input)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x_out = Dense(256, activation='relu')(x)\n",
        "    x_out = Dropout(0.5)(x)\n",
        "    return x_out\n",
        "  \n",
        "def model_2(x_input):\n",
        "    x = Dense(800, activation='sigmoid')(x_input)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(200, activation='sigmoid')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x_out = Dense(12)(x)\n",
        "    return x_out\n",
        "\n",
        "inputs = Input(shape=(784,))\n",
        "x      = model_1(inputs)\n",
        "x_out  = Dense(10, use_bias=False, activation='linear', name='svm')(x)\n",
        "model = Model(inputs, x_out)\n",
        "\n",
        "def svm_loss(layer):\n",
        "    weights = layer.weights[0]\n",
        "    weights_tf = tf.convert_to_tensor(weights)\n",
        "    \n",
        "    def categorical_hinge_loss(y_true, y_pred):\n",
        "        pos = K.sum(y_true * y_pred, axis=-1)\n",
        "        neg = K.max((1.0 - y_true) * y_pred, axis=-1)\n",
        "        hinge_loss = K.mean(K.maximum(0.0, neg - pos + 1), axis=-1)\n",
        "        regularization_loss = 0.5*(tf.reduce_sum(tf.square(weights_tf)))\n",
        "        return regularization_loss + 0.4*hinge_loss\n",
        "    \n",
        "    return categorical_hinge_loss\n",
        "\n",
        "metrics = ['accuracy']\n",
        "optimizer = tf.keras.optimizers.SGD(lr=0.1, momentum=0.9)\n",
        "model.compile(optimizer=optimizer, loss=svm_loss(model.get_layer('svm')), metrics=metrics)\n",
        "\n",
        "epochs = 4\n",
        "batch_size = [16, 32, 64, 128, 200, 256]\n",
        "best_batch_size = []\n",
        "\n",
        "for i in batch_size:\n",
        "  x = model.fit(x_train, y_train,\n",
        "                      batch_size=i,\n",
        "                      epochs=epochs,\n",
        "                      verbose=1,\n",
        "                      validation_data=(x_test, y_test))\n",
        "  score = model.evaluate(x_test, y_test, verbose=0)\n",
        "  best_batch_size.append(score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKnwn7pIq4kh",
        "outputId": "b6f6e6d1-f156-41f5-f800-3ce98ac778f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000 train samples\n",
            "10000 test samples\n",
            "Epoch 1/4\n",
            "3750/3750 [==============================] - 33s 9ms/step - loss: 10.0505 - accuracy: 0.8953 - val_loss: 9.9905 - val_accuracy: 0.9577\n",
            "Epoch 2/4\n",
            "3750/3750 [==============================] - 32s 9ms/step - loss: 9.9958 - accuracy: 0.9537 - val_loss: 9.9776 - val_accuracy: 0.9694\n",
            "Epoch 3/4\n",
            "3750/3750 [==============================] - 31s 8ms/step - loss: 9.9843 - accuracy: 0.9656 - val_loss: 9.9749 - val_accuracy: 0.9721\n",
            "Epoch 4/4\n",
            "3750/3750 [==============================] - 33s 9ms/step - loss: 9.9767 - accuracy: 0.9723 - val_loss: 9.9720 - val_accuracy: 0.9762\n",
            "Epoch 1/4\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 9.9685 - accuracy: 0.9804 - val_loss: 9.9694 - val_accuracy: 0.9772\n",
            "Epoch 2/4\n",
            "1875/1875 [==============================] - 20s 11ms/step - loss: 9.9655 - accuracy: 0.9835 - val_loss: 9.9674 - val_accuracy: 0.9788\n",
            "Epoch 3/4\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 9.9635 - accuracy: 0.9856 - val_loss: 9.9669 - val_accuracy: 0.9794\n",
            "Epoch 4/4\n",
            "1875/1875 [==============================] - 21s 11ms/step - loss: 9.9620 - accuracy: 0.9866 - val_loss: 9.9676 - val_accuracy: 0.9793\n",
            "Epoch 1/4\n",
            "938/938 [==============================] - 13s 14ms/step - loss: 9.9597 - accuracy: 0.9890 - val_loss: 9.9659 - val_accuracy: 0.9811\n",
            "Epoch 2/4\n",
            "938/938 [==============================] - 12s 13ms/step - loss: 9.9584 - accuracy: 0.9905 - val_loss: 9.9657 - val_accuracy: 0.9819\n",
            "Epoch 3/4\n",
            "938/938 [==============================] - 12s 13ms/step - loss: 9.9579 - accuracy: 0.9909 - val_loss: 9.9658 - val_accuracy: 0.9811\n",
            "Epoch 4/4\n",
            "938/938 [==============================] - 12s 13ms/step - loss: 9.9572 - accuracy: 0.9915 - val_loss: 9.9653 - val_accuracy: 0.9822\n",
            "Epoch 1/4\n",
            "469/469 [==============================] - 9s 19ms/step - loss: 9.9562 - accuracy: 0.9927 - val_loss: 9.9646 - val_accuracy: 0.9833\n",
            "Epoch 2/4\n",
            "469/469 [==============================] - 8s 17ms/step - loss: 9.9555 - accuracy: 0.9934 - val_loss: 9.9644 - val_accuracy: 0.9831\n",
            "Epoch 3/4\n",
            "469/469 [==============================] - 9s 19ms/step - loss: 9.9555 - accuracy: 0.9927 - val_loss: 9.9644 - val_accuracy: 0.9832\n",
            "Epoch 4/4\n",
            "469/469 [==============================] - 9s 19ms/step - loss: 9.9552 - accuracy: 0.9934 - val_loss: 9.9644 - val_accuracy: 0.9835\n",
            "Epoch 1/4\n",
            "300/300 [==============================] - 7s 24ms/step - loss: 9.9549 - accuracy: 0.9936 - val_loss: 9.9644 - val_accuracy: 0.9834\n",
            "Epoch 2/4\n",
            "300/300 [==============================] - 7s 24ms/step - loss: 9.9544 - accuracy: 0.9942 - val_loss: 9.9646 - val_accuracy: 0.9832\n",
            "Epoch 3/4\n",
            "300/300 [==============================] - 8s 26ms/step - loss: 9.9544 - accuracy: 0.9943 - val_loss: 9.9644 - val_accuracy: 0.9826\n",
            "Epoch 4/4\n",
            "300/300 [==============================] - 7s 23ms/step - loss: 9.9544 - accuracy: 0.9942 - val_loss: 9.9644 - val_accuracy: 0.9833\n",
            "Epoch 1/4\n",
            "235/235 [==============================] - 7s 31ms/step - loss: 9.9538 - accuracy: 0.9948 - val_loss: 9.9645 - val_accuracy: 0.9830\n",
            "Epoch 2/4\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 9.9538 - accuracy: 0.9946 - val_loss: 9.9645 - val_accuracy: 0.9836\n",
            "Epoch 3/4\n",
            "235/235 [==============================] - 8s 32ms/step - loss: 9.9537 - accuracy: 0.9947 - val_loss: 9.9643 - val_accuracy: 0.9837\n",
            "Epoch 4/4\n",
            "235/235 [==============================] - 6s 27ms/step - loss: 9.9535 - accuracy: 0.9950 - val_loss: 9.9643 - val_accuracy: 0.9833\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = ['16', '32', '64', '128', '200', '256']\n",
        "\n",
        "plt.bar(batch_size, best_batch_size)\n",
        "plt.title('Best Batch Size (MNIST/SVM)')\n",
        "plt.xlabel('Batch Sizes')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "AFnmeQga0HYH",
        "outputId": "e1e804c8-6586-42a8-c7a7-9937dd552520"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#best batch size mnist/softmax\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255.\n",
        "x_test /= 255.\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "def model_1(x_input):\n",
        "    x = Dense(512, activation='relu')(x_input)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x_out = Dense(256, activation='relu')(x)\n",
        "    x_out = Dropout(0.5)(x)\n",
        "    return x_out\n",
        "  \n",
        "def model_2(x_input):\n",
        "    x = Dense(800, activation='sigmoid')(x_input)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(200, activation='sigmoid')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x_out = Dense(12)(x)\n",
        "    return x_out\n",
        "\n",
        "inputs = Input(shape=(784,))\n",
        "x      = model_1(inputs)\n",
        "x_out  = Dense(10, use_bias=False, activation='softmax')(x)\n",
        "model = Model(inputs, x_out)\n",
        "optimizer = tf.keras.optimizers.SGD(lr=0.1, momentum=0.9)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=keras.losses.categorical_crossentropy,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "epochs = 4\n",
        "batch_size = [16, 32, 64, 128, 200, 256]\n",
        "best_batch_size = []\n",
        "\n",
        "for i in batch_size:\n",
        "  x = model.fit(x_train, y_train,\n",
        "                      batch_size=i,\n",
        "                      epochs=epochs,\n",
        "                      verbose=1,\n",
        "                      validation_data=(x_test, y_test))\n",
        "  score = model.evaluate(x_test, y_test, verbose=0)\n",
        "  best_batch_size.append(score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIVD4-COq4sF",
        "outputId": "2e9560e1-2e77-45ef-ac0f-c92d538332fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000 train samples\n",
            "10000 test samples\n",
            "Epoch 1/4\n",
            "3750/3750 [==============================] - 35s 9ms/step - loss: 0.3027 - accuracy: 0.9086 - val_loss: 0.1235 - val_accuracy: 0.9606\n",
            "Epoch 2/4\n",
            "3750/3750 [==============================] - 32s 9ms/step - loss: 0.1384 - accuracy: 0.9590 - val_loss: 0.0975 - val_accuracy: 0.9704\n",
            "Epoch 3/4\n",
            "3750/3750 [==============================] - 32s 8ms/step - loss: 0.0992 - accuracy: 0.9704 - val_loss: 0.0871 - val_accuracy: 0.9759\n",
            "Epoch 4/4\n",
            "3750/3750 [==============================] - 33s 9ms/step - loss: 0.0817 - accuracy: 0.9749 - val_loss: 0.0765 - val_accuracy: 0.9779\n",
            "Epoch 1/4\n",
            "1875/1875 [==============================] - 20s 11ms/step - loss: 0.0466 - accuracy: 0.9857 - val_loss: 0.0611 - val_accuracy: 0.9830\n",
            "Epoch 2/4\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0372 - accuracy: 0.9885 - val_loss: 0.0641 - val_accuracy: 0.9820\n",
            "Epoch 3/4\n",
            "1875/1875 [==============================] - 20s 11ms/step - loss: 0.0300 - accuracy: 0.9910 - val_loss: 0.0649 - val_accuracy: 0.9814\n",
            "Epoch 4/4\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0254 - accuracy: 0.9920 - val_loss: 0.0608 - val_accuracy: 0.9838\n",
            "Epoch 1/4\n",
            "938/938 [==============================] - 12s 13ms/step - loss: 0.0184 - accuracy: 0.9944 - val_loss: 0.0589 - val_accuracy: 0.9845\n",
            "Epoch 2/4\n",
            "938/938 [==============================] - 12s 13ms/step - loss: 0.0145 - accuracy: 0.9957 - val_loss: 0.0586 - val_accuracy: 0.9851\n",
            "Epoch 3/4\n",
            "938/938 [==============================] - 12s 13ms/step - loss: 0.0141 - accuracy: 0.9959 - val_loss: 0.0623 - val_accuracy: 0.9839\n",
            "Epoch 4/4\n",
            "938/938 [==============================] - 12s 13ms/step - loss: 0.0118 - accuracy: 0.9966 - val_loss: 0.0619 - val_accuracy: 0.9830\n",
            "Epoch 1/4\n",
            "469/469 [==============================] - 14s 29ms/step - loss: 0.0102 - accuracy: 0.9971 - val_loss: 0.0607 - val_accuracy: 0.9850\n",
            "Epoch 2/4\n",
            "469/469 [==============================] - 9s 18ms/step - loss: 0.0090 - accuracy: 0.9975 - val_loss: 0.0617 - val_accuracy: 0.9846\n",
            "Epoch 3/4\n",
            "469/469 [==============================] - 9s 20ms/step - loss: 0.0086 - accuracy: 0.9974 - val_loss: 0.0614 - val_accuracy: 0.9845\n",
            "Epoch 4/4\n",
            "469/469 [==============================] - 9s 20ms/step - loss: 0.0082 - accuracy: 0.9979 - val_loss: 0.0626 - val_accuracy: 0.9843\n",
            "Epoch 1/4\n",
            "300/300 [==============================] - 7s 24ms/step - loss: 0.0072 - accuracy: 0.9983 - val_loss: 0.0628 - val_accuracy: 0.9844\n",
            "Epoch 2/4\n",
            "300/300 [==============================] - 9s 30ms/step - loss: 0.0071 - accuracy: 0.9982 - val_loss: 0.0647 - val_accuracy: 0.9852\n",
            "Epoch 3/4\n",
            "300/300 [==============================] - 8s 26ms/step - loss: 0.0067 - accuracy: 0.9981 - val_loss: 0.0638 - val_accuracy: 0.9847\n",
            "Epoch 4/4\n",
            "300/300 [==============================] - 10s 32ms/step - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.0624 - val_accuracy: 0.9850\n",
            "Epoch 1/4\n",
            "235/235 [==============================] - 12s 51ms/step - loss: 0.0062 - accuracy: 0.9984 - val_loss: 0.0620 - val_accuracy: 0.9851\n",
            "Epoch 2/4\n",
            "235/235 [==============================] - 12s 53ms/step - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.0631 - val_accuracy: 0.9848\n",
            "Epoch 3/4\n",
            "235/235 [==============================] - 9s 40ms/step - loss: 0.0061 - accuracy: 0.9984 - val_loss: 0.0631 - val_accuracy: 0.9855\n",
            "Epoch 4/4\n",
            "235/235 [==============================] - 8s 34ms/step - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.0624 - val_accuracy: 0.9853\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = ['16', '32', '64', '128', '200', '256']\n",
        "\n",
        "plt.bar(batch_size, best_batch_size)\n",
        "plt.title('Best Batch Size (MNIST/Softmax)')\n",
        "plt.xlabel('Batch Sizes')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "zR1GSswe86C1",
        "outputId": "e0de274a-3a78-4b00-ea73-c5358f221857"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#best batch size cifar/svm\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "x_train = x_train.reshape(50000, 32*32*3)\n",
        "x_test = x_test.reshape(10000, 32*32*3)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255.\n",
        "x_test /= 255.\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "def model_1(x_input):\n",
        "    x = Dense(512, activation='relu')(x_input)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x_out = Dense(256, activation='relu')(x)\n",
        "    x_out = Dropout(0.2)(x)\n",
        "    return x_out\n",
        "  \n",
        "def model_2(x_input):\n",
        "    x = Dense(800, activation='sigmoid')(x_input)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Dense(200, activation='sigmoid')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x_out = Dense(12)(x)\n",
        "    return x_out\n",
        "\n",
        "inputs = Input(shape=(32*32*3,))\n",
        "x      = model_1(inputs)\n",
        "x_out  = Dense(10, use_bias=False, activation='linear', name='svm')(x)\n",
        "model = Model(inputs, x_out)\n",
        "\n",
        "def svm_loss(layer):\n",
        "    weights = layer.weights[0]\n",
        "    weights_tf = tf.convert_to_tensor(weights)\n",
        "    \n",
        "    def categorical_hinge_loss(y_true, y_pred):\n",
        "        pos = K.sum(y_true * y_pred, axis=-1)\n",
        "        neg = K.max((1.0 - y_true) * y_pred, axis=-1)\n",
        "        hinge_loss = K.mean(K.maximum(0.0, neg - pos + 1), axis=-1)\n",
        "        regularization_loss = 0.5*(tf.reduce_sum(tf.square(weights_tf)))\n",
        "        return regularization_loss + 0.4*hinge_loss\n",
        "    \n",
        "    return categorical_hinge_loss\n",
        "\n",
        "metrics = ['accuracy']\n",
        "optimizer = tf.keras.optimizers.legacy.RMSprop(lr=2e-3, decay=1e-5)\n",
        "#optimizer = tf.train.AdamOptimizer(1.e-3)\n",
        "\n",
        "model.compile(optimizer='adam', loss=svm_loss(model.get_layer('svm')), metrics=metrics)\n",
        "\n",
        "epochs = 4\n",
        "batch_size = [16, 32, 64, 128, 200, 256]\n",
        "best_batch_size = []\n",
        "test_error = []\n",
        "\n",
        "for i in batch_size:\n",
        "  x = model.fit(x_train, y_train,\n",
        "                      batch_size=i,\n",
        "                      epochs=epochs,\n",
        "                      verbose=1,\n",
        "                      validation_data=(x_test, y_test))\n",
        "  score = model.evaluate(x_test, y_test, verbose=0)\n",
        "  best_batch_size.append(score[1])\n",
        "  test_error.append(score[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4N7nnNKNq4zA",
        "outputId": "dfb86bed-0276-41bd-f2bd-a54b43638901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50000 train samples\n",
            "10000 test samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/optimizers/legacy/rmsprop.py:143: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3125/3125 [==============================] - 81s 25ms/step - loss: 10.1364 - accuracy: 0.1006 - val_loss: 10.1279 - val_accuracy: 0.1000\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 10.1279 - accuracy: 0.1005 - val_loss: 10.1279 - val_accuracy: 0.1001\n",
            "782/782 [==============================] - 26s 33ms/step - loss: 10.1279 - accuracy: 0.1000 - val_loss: 10.1279 - val_accuracy: 0.1000\n",
            "391/391 [==============================] - 19s 48ms/step - loss: 10.1279 - accuracy: 0.1000 - val_loss: 10.1279 - val_accuracy: 0.1000\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 10.1279 - accuracy: 0.0999 - val_loss: 10.1279 - val_accuracy: 0.1000\n",
            "196/196 [==============================] - 13s 68ms/step - loss: 10.1279 - accuracy: 0.1000 - val_loss: 10.1279 - val_accuracy: 0.1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = ['16', '32', '64', '128', '200', '256']\n",
        "\n",
        "plt.bar(batch_size, best_batch_size)\n",
        "plt.title('Best Batch Size (Cifar/SVM)')\n",
        "plt.xlabel('Batch Sizes')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "KlOvF9Yb-k91",
        "outputId": "7be1c19e-e90b-4dd6-d895-ad0f2ee90062"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAHHCAYAAABXx+fLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABFiElEQVR4nO3deVwV9f7H8Tc7CGoKAq5g7gtpbohatqCYu7ll3iQss66oZVFqLpUV5q9MU69eK7U0r6alWRlluORNylwwyTI1DVMByQSXAoTv7w8fntuJI4oBR53X8/GYR/Gdz8z5zAD6duY757gYY4wAAAAsxNXZDQAAAJQ1AhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhCAMrNx40a5uLho5cqVpf5aoaGhuv/++0v9df7q8OHD8vb21pdffnlF2x86dEguLi5atGiR3XhCQoKaN28ub29vubi46OTJk3+/2WtI27Zt9eSTTzq7DVxHCEBAMS1atEguLi52S2BgoG6//XZ98sknpfa6Z8+e1TPPPKONGzdeVv2FsPHnpXLlymrbtq3eeeedK+7jX//6V6G/nMvS7t271a9fP4WEhMjb21vVq1dXp06dNGvWLKf19GfPPfecwsPD1b59+0LrNm7cqLvvvlvBwcHy9PRUYGCgevTooffff7/Iff76668aMGCAfHx8NGfOHC1evFi+vr4l2vfu3bvl4uKirVu3SpJOnz6tyZMnq2nTpvL19ZW/v7+aN2+u0aNH6+jRo5Kkm266SbVq1VJRn6jUvn17BQUF6dy5c7Zw5+Lioueff95h/eDBg+Xi4iI/Pz+78aeeekpz5sxRWlpaCR0xLM8AKJaFCxcaSea5554zixcvNm+//bb5v//7P9OkSRMjyXz44Yel8rrHjx83kszkyZMvq37Dhg1Gkhk1apRZvHixWbx4sZkxY4aJiIgwkszs2bOvqI8mTZqYjh07XtG2F3pasWLFFW3/5ZdfGk9PT1O3bl0zZcoU8/rrr5tJkyaZzp07mzp16tjV/vHHHyY3N/eKXudKZWRkGA8PD7N06dJC6yZNmmQkmXr16plJkyaZN99800ybNs3cdtttRpJ55513jDHGFBQUmN9//92cO3fOtu0nn3xiJJl169aVWu/x8fEmMDDQFBQUmNzcXHPzzTcbHx8f8/DDD5t58+aZl19+2cTExJiAgACzYcMGY4wxU6dONZLMpk2bHO7z4MGDxsXFxYwcOdL2tSTj7e1tGjduXKj+9OnTxtfX13h7extfX1+7dfn5+SY4ONhMnDixZA8clkUAAorpQgD65ptv7MZPnDhhPDw8zL333lsqr3ulAeivYSMnJ8dUr17dtGvX7or6cGYA6tq1q6lSpYr57bffCq1LT0+/on2WpOnTpxsfHx9z6tQpu/EVK1YYSaZfv34OQ1lCQkKRwfmtt95y+DP3d5w+fdru61tuucVER0cbY4x599137ULZn/3+++8mKyvLGGNMamqqcXFxMcOHD3f4Gi+++KKRZL766itjzP8C0N13320kmeTkZLv6d955x3h4eJgePXoUCkDGGBMbG2tCQkJMQUFBsY8X+CtugQEl5IYbbpCPj4/c3d3txgsKCjRjxgw1adJE3t7eCgoK0vDhw/Xbb7/Z1W3btk1RUVEKCAiQj4+PateuraFDh0o6Py+kSpUqkqRnn33WdhvhmWeeKXafnp6eqlSpUqE+Fy5cqDvuuEOBgYHy8vJS48aNNXfuXLua0NBQfffdd9q0aZOth9tuu822/uTJk3rssccUGhoqLy8v1ahRQ0OGDFFmZmahc/LCCy+oRo0a8vb21p133qn9+/dfsvcDBw6oSZMmuuGGGwqtCwwMLNTrn+cA/fV24J+XQ4cO2ep++OEH9evXT5UrV5a3t7datWqlNWvWXLI3SVq9erXCw8ML3b6ZOHGiKleurAULFsjDw6PQdlFRUerevbukwnOAbrvtNkVHR0uSWrduLRcXF9txbd68Wf3791etWrXk5eWlmjVr6rHHHtPvv/9ut//7779ffn5+OnDggLp27ary5ctr8ODBtvUnT57Uli1b1K1bN0nnz7Mkh7fxvL29VaFCBUlSzZo1deutt2rlypXKy8srVLt06VLVqVNH4eHhduMRERGqXbu2li5dajf+zjvvqEuXLqpcuXKhfUlSp06d9PPPPys5OdnheqA43C9dAsCRrKwsZWZmyhijjIwMzZo1S6dPn9Y//vEPu7rhw4dr0aJFiomJ0ahRo3Tw4EHNnj1bO3fu1JdffikPDw9lZGSoc+fOqlKlisaOHasbbrhBhw4dss0NqVKliubOnatHHnlEffr00d133y3p/ByMSzl16pQtgJw4cUJLly5VSkqK3nzzTbu6uXPnqkmTJurZs6fc3d314Ycf6p///KcKCgo0YsQISdKMGTM0cuRI+fn56emnn5YkBQUFSTo/Z+SWW27R999/r6FDh6pFixbKzMzUmjVr9MsvvyggIMD2WlOnTpWrq6ueeOIJZWVladq0aRo8eLC+/vrrIo8lJCRESUlJSklJUdOmTS957H+2ePHiQmMTJkxQRkaGLbB89913at++vapXr66xY8fK19dX7777rnr37q333ntPffr0uej+8/Ly9M033+iRRx6xG9+3b59++OEHDR06VOXLly9Wz5L09NNPq0GDBpo/f76ee+451a5dW3Xq1JEkrVixQmfPntUjjzwif39/bd26VbNmzdIvv/yiFStW2O3n3LlzioqKUocOHfTyyy+rXLlytnWffvqpXFxc1LlzZ0nnz7Mkvf3225owYYJcXFwu2t/gwYP10EMP6dNPP7WFOOn8nKKUlBRNmjTJ4XaDBg3SkiVLNHXqVLm4uCgzM1OfffaZFi9erISEBIfbtGzZUpL05Zdf6uabb77UqQOK5uxLUMC15sItsL8uXl5eZtGiRXa1mzdvdngrISEhwW581apVl7zFcaW3wP66uLq6mhdeeKFQ/dmzZwuNRUVFmRtvvNFu7GK3wC7McXn//fcLrbtwy+JCT40aNTI5OTm29TNnzjSSzO7du4s8ps8++8y4ubkZNzc3ExERYZ588knz6aefOrytFBISYrul48i0adOMJPP222/bxu68804TFhZm/vjjD7ve27VrZ+rVq1dkb/v37zeSzKxZs+zGP/jgAyPJvPrqq0Vuf8GF20QLFy60jV3stquj71l8fLxxcXExP//8s20sOjraSDJjx451+Jr33Xef3ff07NmzpkGDBkaSCQkJMffff7958803Hd5mPHHihPHy8jKDBg2yGx87dqyRZPbu3Vvo2P7v//7PpKSkGElm8+bNxhhj5syZY/z8/MyZM2dMdHS0w1tgxhjj6elpHnnkEYfrgOLgFhhwhebMmaN169Zp3bp1WrJkiW6//XY9+OCDdk/0rFixQhUrVlSnTp2UmZlpW1q2bCk/Pz9t2LBBkmy3dD766COHtxL+jkmTJtn6XL58uQYNGqSnn35aM2fOtKvz8fGx/f+Fq1sdO3bUTz/9pKysrEu+znvvvadmzZo5vEry1ysIMTEx8vT0tH19yy23SJJ++umnIl+jU6dOSkpKUs+ePbVr1y5NmzZNUVFRql69+mXfppKkDRs2aNy4cRo5cqTuu+8+Seevjq1fv14DBgywXTXLzMzUr7/+qqioKO3bt09Hjhy56D5//fVXSVKlSpXsxrOzsyXpiq7+XMqfv2dnzpxRZmam2rVrJ2OMdu7cWaj+r1enpPO3IxMSEmy3vy7s9+uvv1ZcXJyk808+PvDAA6patapGjhypnJwcW22lSpXUtWtXrVmzRmfOnJEkGWO0bNkytWrVSvXr13fYe5MmTXTTTTfpP//5j6Tzt8t69epld2XKkUqVKhW6pQpcCQIQcIXatGmjyMhIRUZGavDgwfr444/VuHFjxcbGKjc3V9L52x9ZWVkKDAxUlSpV7JbTp08rIyNDktSxY0f17dtXzz77rAICAtSrVy8tXLjQ7i+aKxUWFmbrc8CAAVqyZIm6d++usWPH6vjx47a6L7/8UpGRkfL19dUNN9ygKlWqaPz48ZJ0WQHowIEDl31bqlatWnZfXwgNf50X5Ujr1q31/vvv67ffftPWrVs1btw4nTp1Sv369dOePXsuuf0vv/yigQMHqn379po+fbptfP/+/TLGaOLEiYW+V5MnT5Yk2/erKOYvj4RfmC9z6tSpS25bXKmpqbr//vtVuXJl+fn5qUqVKurYsaOkwt8zd3d31ahRo9A+vvnmGx0/ftwuAElSxYoVNW3aNB06dEiHDh3Sm2++qQYNGmj27NmaMmWKXe3gwYN15swZffDBB5KkLVu26NChQ3bzjBy59957tWLFCu3fv19btmzRvffee8ljNsYUeUsOuFzMAQJKiKurq26//XbNnDlT+/btU5MmTVRQUKDAwMCLvu/OhYnNF94c8KuvvtKHH36oTz/9VEOHDtUrr7yir776qtCk2r/rzjvv1EcffaStW7eqW7duOnDggO688041bNhQ06dPV82aNeXp6am1a9fq1VdfVUFBQYm+vpubm8Pxv4aHonh6eqp169Zq3bq16tevr5iYGK1YscIWVhzJzc1Vv3795OXlpXfffdduIviFY3ziiScUFRXlcPu6detedN/+/v6SCoe4hg0bSjo/J6Yk5efnq1OnTjpx4oSeeuopNWzYUL6+vjpy5Ijuv//+Qt8zLy8vuboW/jfv2rVrFRoaqsaNG1/0tUJCQjR06FD16dNHN954o9555x279/Hp3r27KlasqKVLl+ree+/V0qVL5ebmpnvuuafIYxg0aJDGjRunYcOGyd/f3zYHqSgnT560m08GXCkCEFCCzp07J+n8hGBJqlOnjj7//HO1b9/e7nbFxbRt21Zt27bVCy+8oKVLl2rw4MFatmyZHnzwwRL9V+9f+/zwww+Vk5OjNWvW2F2duXCL7s8u1kedOnWUkpJSYj0WR6tWrSRJx44dK7Ju1KhRSk5O1hdffGGbvH3BjTfeKEny8PBQZGRksXuoVauWfHx8dPDgQbvx+vXrq0GDBvrggw80c+bMEguzu3fv1o8//qi33npLQ4YMsY2vW7euWPv5+OOP1bVr18uqrVSpksPvs5eXl/r166e3335b6enpWrFihe644w4FBwcXub9atWqpffv22rhxox555JFCTyb+1ZEjR5Sbm6tGjRpdVr9AUbgFBpSQvLw8ffbZZ/L09LT9AT1gwADl5+cXumUgnQ8hFz7O4Lfffit09aN58+aSZLsNdmFuREl8BMJHH30kSWrWrJmk/12R+XMPWVlZWrhwYaFtfX19HfbQt29f7dq1S6tWrSq0rjhXdoqyYcMGh/tau3atJKlBgwYX3XbhwoX697//rTlz5qhNmzaF1gcGBuq2227Tv//9b4dB6s+3Cx3x8PBQq1attG3btkLrnn32Wf3666968MEHbeHzzz777DPb9+RyOfqeGWMKze0qSnp6unbs2FHo9teuXbsczrP5+eeftWfPHofnefDgwcrLy9Pw4cN1/PjxS97+uuD555/X5MmTNXLkyEvWbt++XZLUrl27y9o3UBSuAAFX6JNPPtEPP/wg6fzckKVLl2rfvn0aO3asbd5Hx44dNXz4cMXHxys5OVmdO3eWh4eH9u3bpxUrVmjmzJnq16+f3nrrLf3rX/9Snz59VKdOHZ06dUqvv/66KlSoYPvXuY+Pjxo3bqzly5erfv36qly5spo2bXrJeTebN2/WH3/8Ien8RN81a9Zo06ZNuueee2y3Zzp37ixPT0/16NFDw4cP1+nTp/X6668rMDCwUBho2bKl5s6dq+eff15169ZVYGCg7rjjDsXFxWnlypXq37+/hg4dqpYtW9peb968ebaw9XeMHDlSZ8+eVZ8+fdSwYUPl5uZqy5YtWr58uUJDQxUTE+Nwu8zMTP3zn/9U48aN5eXlpSVLltit79Onj3x9fTVnzhx16NBBYWFhGjZsmG688Ualp6crKSlJv/zyi3bt2lVkf7169dLTTz+t7Oxs28+AJA0cOFC7d+/WCy+8oJ07d2rQoEEKCQnRr7/+qoSEBCUmJhZ6T5xLadiwoerUqaMnnnhCR44cUYUKFfTee+9d1jyqC9auXStvb2/dfvvtduPr1q3T5MmT1bNnT7Vt21Z+fn766aeftGDBAuXk5Dh8/6mOHTuqRo0a+uCDD+Tj42N7q4ZL6dixo23e0qWsW7dOtWrV4hF4lAznPHwGXLscPQbv7e1tmjdvbubOnevwXWrnz59vWrZsaXx8fEz58uVNWFiYefLJJ83Ro0eNMcbs2LHDDBo0yNSqVct4eXmZwMBA0717d7Nt2za7/WzZssW0bNnSeHp6XvKReEePwXt6epqGDRuaF154odCj42vWrDE33XST8fb2NqGhoeall14yCxYsMJLMwYMHbXVpaWmmW7dupnz58kaS3ePTv/76q4mNjTXVq1c3np6epkaNGiY6OtpkZmba9fTXd4J29Oi3I5988okZOnSoadiwofHz87N9LMbIkSMLPaL958fgL+z/Ysufj+/AgQNmyJAhJjg42Hh4eJjq1aub7t27m5UrVxbZmzHn343a3d3dLF682OH6xMRE06tXLxMYGGjc3d1NlSpVTI8ePcwHH3xQ5Lm42GPwe/bsMZGRkcbPz88EBASYYcOGmV27dhXa/mKPlffr18907dq10PhPP/1kJk2aZNq2bWvXa7du3cz69esvevxxcXFGkhkwYIDD9X9+DL4ojvrNz883VatWNRMmTChyW+ByuRhTQtemAQB64IEH9OOPP2rz5s3ObqVI586dk7+/v+Lj4/XPf/7T2e1c0urVq3XvvffqwIEDqlq1qrPbwXWAAAQAJSg1NVX169dXYmKiw4+SuFpkZGToX//6l4YPH35NBIqIiAjdcsstmjZtmrNbwXWCAAQAACyHp8AAAIDlEIAAAIDlEIAAAIDlEIAAAIDl8EaIDhQUFOjo0aMqX748H7oHAMA1whijU6dOqVq1ag4/++7PCEAOHD16VDVr1nR2GwAA4AocPnxYNWrUKLKGAORA+fLlJZ0/gX9+O3sAAHD1ys7OVs2aNW1/jxeFAOTAhdteFSpUIAABAHCNuZzpK0yCBgAAlkMAAgAAlkMAAgAAlkMAAgAAlkMAAgAAlkMAAgAAlkMAAgAAlkMAAgAAlkMAAgAAlkMAAgAAluP0ADRnzhyFhobK29tb4eHh2rp160Vrv/vuO/Xt21ehoaFycXHRjBkz/vY+AQCA9Tg1AC1fvlxjxozR5MmTtWPHDjVr1kxRUVHKyMhwWH/27FndeOONmjp1qoKDg0tknwAAwHpcjDHGWS8eHh6u1q1ba/bs2ZKkgoIC1axZUyNHjtTYsWOL3DY0NFSPPvqoHn300RLb5wXZ2dmqWLGisrKy+DBUAACuEcX5+9tpV4Byc3O1fft2RUZG/q8ZV1dFRkYqKSnpqtknAAC4/rg764UzMzOVn5+voKAgu/GgoCD98MMPZbrPnJwc5eTk2L7Ozs6+otcHAADXBqcFoKtJfHy8nn322TJ7vdCxH5fZa11NDk3tdsXbWvWcSZy3K/F3zpnEebtSnLfi45w5j9NugQUEBMjNzU3p6el24+np6Red4Fxa+xw3bpyysrJsy+HDh6/o9QEAwLXBaQHI09NTLVu2VGJiom2soKBAiYmJioiIKNN9enl5qUKFCnYLAAC4fjn1FtiYMWMUHR2tVq1aqU2bNpoxY4bOnDmjmJgYSdKQIUNUvXp1xcfHSzo/yXnPnj22/z9y5IiSk5Pl5+enunXrXtY+AQAAnBqABg4cqOPHj2vSpElKS0tT8+bNlZCQYJvEnJqaKlfX/12kOnr0qG6++Wbb1y+//LJefvlldezYURs3brysfQIAADh9EnRsbKxiY2MdrrsQai4IDQ3V5bxtUVH7BAAAcPpHYQAAAJQ1AhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcAhAAALAcpwegOXPmKDQ0VN7e3goPD9fWrVuLrF+xYoUaNmwob29vhYWFae3atXbrT58+rdjYWNWoUUM+Pj5q3Lix5s2bV5qHAAAArjFODUDLly/XmDFjNHnyZO3YsUPNmjVTVFSUMjIyHNZv2bJFgwYN0gMPPKCdO3eqd+/e6t27t1JSUmw1Y8aMUUJCgpYsWaLvv/9ejz76qGJjY7VmzZqyOiwAAHCVc2oAmj59uoYNG6aYmBjblZpy5cppwYIFDutnzpypLl26KC4uTo0aNdKUKVPUokULzZ4921azZcsWRUdH67bbblNoaKgeeughNWvW7JJXlgAAgHU4LQDl5uZq+/btioyM/F8zrq6KjIxUUlKSw22SkpLs6iUpKirKrr5du3Zas2aNjhw5ImOMNmzYoB9//FGdO3e+aC85OTnKzs62WwAAwPXLaQEoMzNT+fn5CgoKshsPCgpSWlqaw23S0tIuWT9r1iw1btxYNWrUkKenp7p06aI5c+bo1ltvvWgv8fHxqlixom2pWbPm3zgyAABwtXP6JOiSNmvWLH311Vdas2aNtm/frldeeUUjRozQ559/ftFtxo0bp6ysLNty+PDhMuwYAACUNXdnvXBAQIDc3NyUnp5uN56enq7g4GCH2wQHBxdZ//vvv2v8+PFatWqVunXrJkm66aablJycrJdffrnQ7bMLvLy85OXl9XcPCQAAXCOcdgXI09NTLVu2VGJiom2soKBAiYmJioiIcLhNRESEXb0krVu3zlafl5envLw8ubraH5abm5sKCgpK+AgAAMC1ymlXgKTzj6xHR0erVatWatOmjWbMmKEzZ84oJiZGkjRkyBBVr15d8fHxkqTRo0erY8eOeuWVV9StWzctW7ZM27Zt0/z58yVJFSpUUMeOHRUXFycfHx+FhIRo06ZNevvttzV9+nSnHScAALi6ODUADRw4UMePH9ekSZOUlpam5s2bKyEhwTbROTU11e5qTrt27bR06VJNmDBB48ePV7169bR69Wo1bdrUVrNs2TKNGzdOgwcP1okTJxQSEqIXXnhBDz/8cJkfHwAAuDo5NQBJUmxsrGJjYx2u27hxY6Gx/v37q3///hfdX3BwsBYuXFhS7QEAgOvQdfcUGAAAwKUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUQgAAAgOU4PQDNmTNHoaGh8vb2Vnh4uLZu3Vpk/YoVK9SwYUN5e3srLCxMa9euLVTz/fffq2fPnqpYsaJ8fX3VunVrpaamltYhAACAa4xTA9Dy5cs1ZswYTZ48WTt27FCzZs0UFRWljIwMh/VbtmzRoEGD9MADD2jnzp3q3bu3evfurZSUFFvNgQMH1KFDBzVs2FAbN27Ut99+q4kTJ8rb27usDgsAAFzlih2AQkND9dxzz5XIFZXp06dr2LBhiomJUePGjTVv3jyVK1dOCxYscFg/c+ZMdenSRXFxcWrUqJGmTJmiFi1aaPbs2baap59+Wl27dtW0adN08803q06dOurZs6cCAwP/dr8AAOD6UOwA9Oijj+r999/XjTfeqE6dOmnZsmXKyckp9gvn5uZq+/btioyM/F8zrq6KjIxUUlKSw22SkpLs6iUpKirKVl9QUKCPP/5Y9evXV1RUlAIDAxUeHq7Vq1cXuz8AAHD9uqIAlJycrK1bt6pRo0YaOXKkqlatqtjYWO3YseOy95OZman8/HwFBQXZjQcFBSktLc3hNmlpaUXWZ2Rk6PTp05o6daq6dOmizz77TH369NHdd9+tTZs2XbSXnJwcZWdn2y0AAOD6dcVzgFq0aKHXXntNR48e1eTJk/XGG2+odevWat68uRYsWCBjTEn2eVkKCgokSb169dJjjz2m5s2ba+zYserevbvmzZt30e3i4+NVsWJF21KzZs2yahkAADjBFQegvLw8vfvuu+rZs6cef/xxtWrVSm+88Yb69u2r8ePHa/DgwUVuHxAQIDc3N6Wnp9uNp6enKzg42OE2wcHBRdYHBATI3d1djRs3tqtp1KhRkXOWxo0bp6ysLNty+PDhInsHAADXtmIHoB07dtjd9mrSpIlSUlL03//+VzExMZo4caI+//xzrVq1qsj9eHp6qmXLlkpMTLSNFRQUKDExUREREQ63iYiIsKuXpHXr1tnqPT091bp1a+3du9eu5scff1RISMhFe/Hy8lKFChXsFgAAcP1yL+4GrVu3VqdOnTR37lz17t1bHh4ehWpq166te+6555L7GjNmjKKjo9WqVSu1adNGM2bM0JkzZxQTEyNJGjJkiKpXr674+HhJ0ujRo9WxY0e98sor6tatm5YtW6Zt27Zp/vz5tn3GxcVp4MCBuvXWW3X77bcrISFBH374oTZu3FjcQwUAANepYgegn376qcirKZLk6+urhQsXXnJfAwcO1PHjxzVp0iSlpaWpefPmSkhIsE10Tk1Nlavr/y5StWvXTkuXLtWECRM0fvx41atXT6tXr1bTpk1tNX369NG8efMUHx+vUaNGqUGDBnrvvffUoUOH4h4qAAC4ThU7AGVkZCgtLU3h4eF2419//bXc3NzUqlWrYu0vNjZWsbGxDtc5umrTv39/9e/fv8h9Dh06VEOHDi1WHwAAwDqKPQdoxIgRDicJHzlyRCNGjCiRpgAAAEpTsQPQnj171KJFi0LjN998s/bs2VMiTQEAAJSmYgcgLy+vQo+iS9KxY8fk7l7sO2oAAABlrtgBqHPnzrb3zbng5MmTGj9+vDp16lSizQEAAJSGYl+yefnll3XrrbcqJCREN998syQpOTlZQUFBWrx4cYk3CAAAUNKKHYCqV6+ub7/9Vu+884527dolHx8fxcTEaNCgQQ7fEwgAAOBqc0WTdnx9ffXQQw+VdC8AAABl4opnLe/Zs0epqanKzc21G+/Zs+ffbgoAAKA0XdE7Qffp00e7d++Wi4uL7VPfXVxcJEn5+fkl2yEAAEAJK/ZTYKNHj1bt2rWVkZGhcuXK6bvvvtMXX3yhVq1a8XlbAADgmlDsK0BJSUlav369AgIC5OrqKldXV3Xo0MH22Vs7d+4sjT4BAABKTLGvAOXn56t8+fKSpICAAB09elSSFBISor1795ZsdwAAAKWg2FeAmjZtql27dql27doKDw/XtGnT5Onpqfnz5+vGG28sjR4BAABKVLED0IQJE3TmzBlJ0nPPPafu3bvrlltukb+/v5YvX17iDQIAAJS0YgegqKgo2//XrVtXP/zwg06cOKFKlSrZngQDAAC4mhVrDlBeXp7c3d2VkpJiN165cmXCDwAAuGYUKwB5eHioVq1avNcPAAC4phX7KbCnn35a48eP14kTJ0qjHwAAgFJX7DlAs2fP1v79+1WtWjWFhITI19fXbv2OHTtKrDkAAIDSUOwA1Lt371JoAwAAoOwUOwBNnjy5NPoAAAAoM8WeAwQAAHCtK/YVIFdX1yIfeecJMQAAcLUrdgBatWqV3dd5eXnauXOn3nrrLT377LMl1hgAAEBpKXYA6tWrV6Gxfv36qUmTJlq+fLkeeOCBEmkMAACgtJTYHKC2bdsqMTGxpHYHAABQakokAP3+++967bXXVL169ZLYHQAAQKkq9i2wv37oqTFGp06dUrly5bRkyZISbQ4AAKA0FDsAvfrqq3YByNXVVVWqVFF4eLgqVapUos0BAACUhmIHoPvvv78U2gAAACg7xZ4DtHDhQq1YsaLQ+IoVK/TWW2+VSFMAAAClqdgBKD4+XgEBAYXGAwMD9eKLL5ZIUwAAAKWp2AEoNTVVtWvXLjQeEhKi1NTUEmkKAACgNBU7AAUGBurbb78tNL5r1y75+/uXSFMAAAClqdgBaNCgQRo1apQ2bNig/Px85efna/369Ro9erTuueee0ugRAACgRBX7KbApU6bo0KFDuvPOO+Xufn7zgoICDRkyhDlAAADgmlDsAOTp6anly5fr+eefV3Jysnx8fBQWFqaQkJDS6A8AAKDEFTsAXVCvXj3Vq1evJHsBAAAoE8WeA9S3b1+99NJLhcanTZum/v37l0hTAAAApanYAeiLL75Q165dC43fdddd+uKLL0qkKQAAgNJU7AB0+vRpeXp6Fhr38PBQdnZ2iTQFAABQmoodgMLCwrR8+fJC48uWLVPjxo1LpCkAAIDSVOxJ0BMnTtTdd9+tAwcO6I477pAkJSYmaunSpVq5cmWJNwgAAFDSih2AevToodWrV+vFF1/UypUr5ePjo2bNmmn9+vWqXLlyafQIAABQoq7oMfhu3bqpW7dukqTs7Gz95z//0RNPPKHt27crPz+/RBsEAAAoacWeA3TBF198oejoaFWrVk2vvPKK7rjjDn311Vcl2RsAAECpKNYVoLS0NC1atEhvvvmmsrOzNWDAAOXk5Gj16tVMgAYAANeMy74C1KNHDzVo0EDffvutZsyYoaNHj2rWrFml2RsAAECpuOwrQJ988olGjRqlRx55hI/AAAAA17TLvgL03//+V6dOnVLLli0VHh6u2bNnKzMzszR7AwAAKBWXHYDatm2r119/XceOHdPw4cO1bNkyVatWTQUFBVq3bp1OnTpVmn0CAACUmGI/Bebr66uhQ4fqv//9r3bv3q3HH39cU6dOVWBgoHr27FkaPQIAAJSoK34MXpIaNGigadOm6ZdfftF//vOfkuoJAACgVP2tAHSBm5ubevfurTVr1pTE7gAAAEpViQQgAACAawkBCAAAWA4BCAAAWA4BCAAAWA4BCAAAWA4BCAAAWM5VEYDmzJmj0NBQeXt7Kzw8XFu3bi2yfsWKFWrYsKG8vb0VFhamtWvXXrT24YcflouLi2bMmFHCXQMAgGuV0wPQ8uXLNWbMGE2ePFk7duxQs2bNFBUVpYyMDIf1W7Zs0aBBg/TAAw9o586d6t27t3r37q2UlJRCtatWrdJXX32latWqlfZhAACAa4jTA9D06dM1bNgwxcTEqHHjxpo3b57KlSunBQsWOKyfOXOmunTpori4ODVq1EhTpkxRixYtNHv2bLu6I0eOaOTIkXrnnXfk4eFRFocCAACuEU4NQLm5udq+fbsiIyNtY66uroqMjFRSUpLDbZKSkuzqJSkqKsquvqCgQPfdd5/i4uLUpEmTS/aRk5Oj7OxsuwUAAFy/nBqAMjMzlZ+fr6CgILvxoKAgpaWlOdwmLS3tkvUvvfSS3N3dNWrUqMvqIz4+XhUrVrQtNWvWLOaRAACAa4nTb4GVtO3bt2vmzJlatGiRXFxcLmubcePGKSsry7YcPny4lLsEAADO5NQAFBAQIDc3N6Wnp9uNp6enKzg42OE2wcHBRdZv3rxZGRkZqlWrltzd3eXu7q6ff/5Zjz/+uEJDQx3u08vLSxUqVLBbAADA9cupAcjT01MtW7ZUYmKibaygoECJiYmKiIhwuE1ERIRdvSStW7fOVn/ffffp22+/VXJysm2pVq2a4uLi9Omnn5bewQAAgGuGu7MbGDNmjKKjo9WqVSu1adNGM2bM0JkzZxQTEyNJGjJkiKpXr674+HhJ0ujRo9WxY0e98sor6tatm5YtW6Zt27Zp/vz5kiR/f3/5+/vbvYaHh4eCg4PVoEGDsj04AABwVXJ6ABo4cKCOHz+uSZMmKS0tTc2bN1dCQoJtonNqaqpcXf93oapdu3ZaunSpJkyYoPHjx6tevXpavXq1mjZt6qxDAAAA1xinByBJio2NVWxsrMN1GzduLDTWv39/9e/f/7L3f+jQoSvsDAAAXI+uu6fAAAAALoUABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALIcABAAALOeqCEBz5sxRaGiovL29FR4erq1btxZZv2LFCjVs2FDe3t4KCwvT2rVrbevy8vL01FNPKSwsTL6+vqpWrZqGDBmio0ePlvZhAACAa4TTA9Dy5cs1ZswYTZ48WTt27FCzZs0UFRWljIwMh/VbtmzRoEGD9MADD2jnzp3q3bu3evfurZSUFEnS2bNntWPHDk2cOFE7duzQ+++/r71796pnz55leVgAAOAq5vQANH36dA0bNkwxMTFq3Lix5s2bp3LlymnBggUO62fOnKkuXbooLi5OjRo10pQpU9SiRQvNnj1bklSxYkWtW7dOAwYMUIMGDdS2bVvNnj1b27dvV2pqalkeGgAAuEo5NQDl5uZq+/btioyMtI25uroqMjJSSUlJDrdJSkqyq5ekqKioi9ZLUlZWllxcXHTDDTc4XJ+Tk6Ps7Gy7BQAAXL+cGoAyMzOVn5+voKAgu/GgoCClpaU53CYtLa1Y9X/88YeeeuopDRo0SBUqVHBYEx8fr4oVK9qWmjVrXsHRAACAa4XTb4GVpry8PA0YMEDGGM2dO/eidePGjVNWVpZtOXz4cBl2CQAAypq7M188ICBAbm5uSk9PtxtPT09XcHCww22Cg4Mvq/5C+Pn555+1fv36i179kSQvLy95eXld4VEAAIBrjVOvAHl6eqply5ZKTEy0jRUUFCgxMVEREREOt4mIiLCrl6R169bZ1V8IP/v27dPnn38uf3//0jkAAABwTXLqFSBJGjNmjKKjo9WqVSu1adNGM2bM0JkzZxQTEyNJGjJkiKpXr674+HhJ0ujRo9WxY0e98sor6tatm5YtW6Zt27Zp/vz5ks6Hn379+mnHjh366KOPlJ+fb5sfVLlyZXl6ejrnQAEAwFXD6QFo4MCBOn78uCZNmqS0tDQ1b95cCQkJtonOqampcnX934Wqdu3aaenSpZowYYLGjx+vevXqafXq1WratKkk6ciRI1qzZo0kqXnz5navtWHDBt12221lclwAAODq5fQAJEmxsbGKjY11uG7jxo2Fxvr376/+/fs7rA8NDZUxpiTbAwAA15nr+ikwAAAARwhAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcghAAADAcq6KADRnzhyFhobK29tb4eHh2rp1a5H1K1asUMOGDeXt7a2wsDCtXbvWbr0xRpMmTVLVqlXl4+OjyMhI7du3rzQPAQAAXEOcHoCWL1+uMWPGaPLkydqxY4eaNWumqKgoZWRkOKzfsmWLBg0apAceeEA7d+5U79691bt3b6WkpNhqpk2bptdee03z5s3T119/LV9fX0VFRemPP/4oq8MCAABXMacHoOnTp2vYsGGKiYlR48aNNW/ePJUrV04LFixwWD9z5kx16dJFcXFxatSokaZMmaIWLVpo9uzZks5f/ZkxY4YmTJigXr166aabbtLbb7+to0ePavXq1WV4ZAAA4Grl1ACUm5ur7du3KzIy0jbm6uqqyMhIJSUlOdwmKSnJrl6SoqKibPUHDx5UWlqaXU3FihUVHh5+0X0CAABrcXfmi2dmZio/P19BQUF240FBQfrhhx8cbpOWluawPi0tzbb+wtjFav4qJydHOTk5tq+zsrIkSdnZ2cU4mstXkHO2VPZ7tfs759Oq50zivF2Jv/u7y3m7Mpy34uOclc5+jTGXrHVqALpaxMfH69lnny00XrNmTSd0c/2qOMPZHVybOG/Fxzm7Mpy3K8N5K77SPmenTp1SxYoVi6xxagAKCAiQm5ub0tPT7cbT09MVHBzscJvg4OAi6y/8Nz09XVWrVrWrad68ucN9jhs3TmPGjLF9XVBQoBMnTsjf318uLi7FPq6rVXZ2tmrWrKnDhw+rQoUKzm7nmsA5uzKctyvDebsynLfiu17PmTFGp06dUrVq1S5Z69QA5OnpqZYtWyoxMVG9e/eWdD58JCYmKjY21uE2ERERSkxM1KOPPmobW7dunSIiIiRJtWvXVnBwsBITE22BJzs7W19//bUeeeQRh/v08vKSl5eX3dgNN9zwt47talahQoXr6ge+LHDOrgzn7cpw3q4M5634rsdzdqkrPxc4/RbYmDFjFB0drVatWqlNmzaaMWOGzpw5o5iYGEnSkCFDVL16dcXHx0uSRo8erY4dO+qVV15Rt27dtGzZMm3btk3z58+XJLm4uOjRRx/V888/r3r16ql27dqaOHGiqlWrZgtZAADA2pwegAYOHKjjx49r0qRJSktLU/PmzZWQkGCbxJyamipX1/89rNauXTstXbpUEyZM0Pjx41WvXj2tXr1aTZs2tdU8+eSTOnPmjB566CGdPHlSHTp0UEJCgry9vcv8+AAAwNXHxVzOVGlcF3JychQfH69x48YVuuUHxzhnV4bzdmU4b1eG81Z8nDMCEAAAsCCnvxM0AABAWSMAAQAAyyEAAQAAyyEAAQAAyyEAXWe++OIL9ejRQ9WqVZOLi4tWr15dqOb7779Xz549VbFiRfn6+qp169ZKTU0t+2avInPnztVNN91ke1OwiIgIffLJJ5KkEydOaOTIkWrQoIF8fHxUq1YtjRo1yvaZcVZ25MgR/eMf/5C/v798fHwUFhambdu2Oax9+OGH5eLiohkzZpRtk05W1O9kXl6ennrqKYWFhcnX11fVqlXTkCFDdPToUbt9/Pjjj+rVq5cCAgJUoUIFdejQQRs2bCjjIylb8fHxat26tcqXL6/AwED17t1be/futav5448/NGLECPn7+8vPz099+/Yt9EkBqamp6tatm8qVK6fAwEDFxcXp3LlzZXkoZeZyztltt90mFxcXu+Xhhx8utK9Fixbppptukre3twIDAzVixIiyOowyQwC6zpw5c0bNmjXTnDlzHK4/cOCAOnTooIYNG2rjxo369ttvNXHiRMu/R1KNGjU0depUbd++Xdu2bdMdd9yhXr166bvvvtPRo0d19OhRvfzyy0pJSdGiRYuUkJCgBx54wNltO9Vvv/2m9u3by8PDQ5988on27NmjV155RZUqVSpUu2rVKn311VeX9fb015uififPnj2rHTt2aOLEidqxY4fef/997d27Vz179rSr6969u86dO6f169dr+/btatasmbp3737RD3i+HmzatEkjRozQV199pXXr1ikvL0+dO3fWmTNnbDWPPfaYPvzwQ61YsUKbNm3S0aNHdffdd9vW5+fnq1u3bsrNzdWWLVv01ltvadGiRZo0aZIzDqnUXc45k6Rhw4bp2LFjtmXatGl266dPn66nn35aY8eO1XfffafPP/9cUVFRZXkoZcPguiXJrFq1ym5s4MCB5h//+IdzGrrGVKpUybzxxhsO17377rvG09PT5OXllXFXV4+nnnrKdOjQ4ZJ1v/zyi6levbpJSUkxISEh5tVXXy395q5Sjn4n/2rr1q1Gkvn555+NMcYcP37cSDJffPGFrSY7O9tIMuvWrSvNdq8qGRkZRpLZtGmTMcaYkydPGg8PD7NixQpbzffff28kmaSkJGOMMWvXrjWurq4mLS3NVjN37lxToUIFk5OTU7YH4AR/PWfGGNOxY0czevToi25z4sQJ4+PjYz7//PMy6NC5uAJkIQUFBfr4449Vv359RUVFKTAwUOHh4Q5vk1lZfn6+li1bpjNnztg+Y+6vsrKyVKFCBbm7O/3N1J1mzZo1atWqlfr376/AwEDdfPPNev311+1qCgoKdN999ykuLk5NmjRxUqfXlqysLLm4uNg+j9Df318NGjTQ22+/rTNnzujcuXP697//rcDAQLVs2dK5zZahC7ecK1euLEnavn278vLyFBkZaatp2LChatWqpaSkJElSUlKSwsLCbJ8sIElRUVHKzs7Wd999V4bdO8dfz9kF77zzjgICAtS0aVONGzdOZ8+eta1bt26dCgoKdOTIETVq1Eg1atTQgAEDdPjw4TLtvSwQgCwkIyNDp0+f1tSpU9WlSxd99tln6tOnj+6++25t2rTJ2e053e7du+Xn5ycvLy89/PDDWrVqlRo3blyoLjMzU1OmTNFDDz3khC6vHj/99JPmzp2revXq6dNPP9UjjzyiUaNG6a233rLVvPTSS3J3d9eoUaOc2Om1448//tBTTz2lQYMG2T6g0sXFRZ9//rl27typ8uXLy9vbW9OnT1dCQoLD243Xo4KCAj366KNq37697WOP0tLS5OnpWeiDq4OCgmy3BtPS0uzCz4X1F9ZdzxydM0m69957tWTJEm3YsEHjxo3T4sWL9Y9//MO2/qefflJBQYFefPFFzZgxQytXrtSJEyfUqVMn5ebmOuNQSo11//lqQQUFBZKkXr166bHHHpMkNW/eXFu2bNG8efPUsWNHZ7bndA0aNFBycrKysrK0cuVKRUdHa9OmTXYhKDs7W926dVPjxo31zDPPOK/Zq0BBQYFatWqlF198UZJ08803KyUlRfPmzVN0dLS2b9+umTNnaseOHXJxcXFyt1e/vLw8DRgwQMYYzZ071zZujNGIESMUGBiozZs3y8fHR2+88YZ69Oihb775RlWrVnVi12VjxIgRSklJ0X//+19nt3LNuNg5+/M/3MLCwlS1alXdeeedOnDggOrUqaOCggLl5eXptddeU+fOnSVJ//nPfxQcHKwNGzZcV3OBuAJkIQEBAXJ3dy90VaNRo0aWfwpMkjw9PVW3bl21bNlS8fHxatasmWbOnGlbf+rUKXXp0kXly5fXqlWr5OHh4cRuna9q1apF/ixt3rxZGRkZqlWrltzd3eXu7q6ff/5Zjz/+uEJDQ53Q8dXrQvj5+eeftW7dOtvVH0lav369PvroIy1btkzt27dXixYt9K9//Us+Pj52V9uuV7Gxsfroo4+0YcMG1ahRwzYeHBys3NxcnTx50q4+PT1dwcHBtpq/PhV24esLNdeji50zR8LDwyVJ+/fvlyRboP7z73aVKlUUEBBw3f09QQCyEE9PT7Vu3brQY5E//vijQkJCnNTV1augoEA5OTmSzl/56dy5szw9PbVmzRrLPzUnSe3bty/yZ+m+++7Tt99+q+TkZNtSrVo1xcXF6dNPP3VGy1elC+Fn3759+vzzz+Xv72+3/sL8DFdX+z+uXV1dbVd1r0fGGMXGxmrVqlVav369ateubbe+ZcuW8vDwUGJiom1s7969Sk1Ntc3di4iI0O7du5WRkWGruRAwHd3evtZd6pw5kpycLOl/wad9+/aSZPe7feLECWVmZl5/f084dw42StqpU6fMzp07zc6dO40kM336dLNz507bEyXvv/++8fDwMPPnzzf79u0zs2bNMm5ubmbz5s1O7ty5xo4dazZt2mQOHjxovv32WzN27Fjj4uJiPvvsM5OVlWXCw8NNWFiY2b9/vzl27JhtOXfunLNbd5qtW7cad3d388ILL5h9+/aZd955x5QrV84sWbLkottY8Smwon4nc3NzTc+ePU2NGjVMcnKy3c/WhaeUjh8/bvz9/c3dd99tkpOTzd69e80TTzxhPDw8THJyspOPrvQ88sgjpmLFimbjxo125+Xs2bO2mocfftjUqlXLrF+/3mzbts1ERESYiIgI2/pz586Zpk2bms6dO5vk5GSTkJBgqlSpYsaNG+eMQyp1lzpn+/fvN88995zZtm2bOXjwoPnggw/MjTfeaG699Va7/fTq1cs0adLEfPnll2b37t2me/fupnHjxiY3N9cZh1VqCEDXmQ0bNhhJhZbo6GhbzZtvvmnq1q1rvL29TbNmzczq1aud1/BVYujQoSYkJMR4enqaKlWqmDvvvNN89tlnxpiLn1NJ5uDBg85t3Mk+/PBD07RpU+Pl5WUaNmxo5s+fX2S9FQNQUb+TBw8evOjP1oYNG2z7+Oabb0znzp1N5cqVTfny5U3btm3N2rVrnXdQZeBi52XhwoW2mt9//93885//NJUqVTLlypUzffr0MceOHbPbz6FDh8xdd91lfHx8TEBAgHn88cev27evuNQ5S01NNbfeequpXLmy8fLyMnXr1jVxcXEmKyvLbj9ZWVlm6NCh5oYbbjCVK1c2ffr0MampqU44otLlYowxpXuNCQAA4OrCHCAAAGA5BCAAAGA5BCAAAGA5BCAAAGA5BCAAAGA5BCAAAGA5BCAAAGA5BCAA151FixYV+pTwkvDMM8+oefPmJb5fAGWPAASgVNx///1ycXGxLf7+/urSpYu+/fbbYu2nLEPHqlWr1LZtW1WsWFHly5dXkyZN9Oijj9rWP/HEE3afPQXg2kUAAlBqunTpomPHjunYsWNKTEyUu7u7unfv7uy2HEpMTNTAgQPVt29fbd26Vdu3b9cLL7ygvLw8W42fn1+hDysFcG0iAAEoNV5eXgoODlZwcLCaN2+usWPH6vDhwzp+/Lit5qmnnlL9+vVVrlw53XjjjZo4caItdCxatEjPPvusdu3aZbuStGjRIknSyZMnNXz4cAUFBcnb21tNmzbVRx99ZPf6n376qRo1aiQ/Pz9bGLuYDz/8UO3bt1dcXJwaNGig+vXrq3fv3pozZ46t5q9Xo/58hevCEhoaalufkpKiu+66S35+fgoKCtJ9992nzMxM2/qVK1cqLCxMPj4+8vf3V2RkpM6cOXMlpxpAMRGAAJSJ06dPa8mSJapbt67dVZTy5ctr0aJF2rNnj2bOnKnXX39dr776qiRp4MCBevzxx9WkSRPblaSBAweqoKBAd911l7788kstWbJEe/bs0dSpU+Xm5mbb79mzZ/Xyyy9r8eLF+uKLL5Samqonnnjiov0FBwfru+++U0pKymUf04Wejh07pv3796tu3bq69dZbJZ0PaHfccYduvvlmbdu2TQkJCUpPT9eAAQNs2w4aNEhDhw7V999/r40bN+ruu+8WH88IlBEnfxgrgOtUdHS0cXNzM76+vsbX19dIMlWrVjXbt28vcrv/+7//My1btrR9PXnyZNOsWTO7mk8//dS4urqavXv3OtzHwoULjSSzf/9+29icOXNMUFDQRV/39OnTpmvXrkaSCQkJMQMHDjRvvvmm+eOPP4rsxRhjCgoKTJ8+fUzLli3N2bNnjTHGTJkyxXTu3Nmu7vDhw0aS2bt3r9m+fbuRZA4dOnTRngCUHq4AASg1t99+u5KTk5WcnKytW7cqKipKd911l37++WdbzfLly9W+fXsFBwfLz89PEyZMUGpqapH7TU5OVo0aNVS/fv2L1pQrV0516tSxfV21alVlZGRctN7X11cff/yx9u/frwkTJsjPz0+PP/642rRpo7NnzxbZz/jx45WUlKQPPvhAPj4+kqRdu3Zpw4YN8vPzsy0NGzaUJB04cEDNmjXTnXfeqbCwMPXv31+vv/66fvvttyJfB0DJIQABKDW+vr6qW7eu6tatq9atW+uNN97QmTNn9Prrr0uSkpKSNHjwYHXt2lUfffSRdu7cqaefflq5ublF7vdCyCiKh4eH3dcuLi6XdXupTp06evDBB/XGG29ox44d2rNnj5YvX37R+iVLlujVV1/VqlWrVL16ddv46dOn1aNHD1sAvLDs27dPt956q9zc3LRu3Tp98sknaty4sWbNmqUGDRro4MGDl+wRwN/n7uwGAFiHi4uLXF1d9fvvv0uStmzZopCQED399NO2mj9fHZIkT09P5efn243ddNNN+uWXX/Tjjz8WeRXo7woNDVW5cuUuOjE5KSlJDz74oP7973+rbdu2dutatGih9957T6GhoXJ3d/xHrYuLi9q3b6/27dtr0qRJCgkJ0apVqzRmzJgSPxYA9ghAAEpNTk6O0tLSJEm//fabZs+ebbsyIkn16tVTamqqli1bptatW+vjjz/WqlWr7PYRGhqqgwcP2m57lS9fXh07dtStt96qvn37avr06apbt65++OEHubi4qEuXLlfU6zPPPKOzZ8+qa9euCgkJ0cmTJ/Xaa68pLy9PnTp1KlSflpamPn366J577lFUVJTtON3c3FSlShWNGDFCr7/+ugYNGqQnn3xSlStX1v79+7Vs2TK98cYb2rZtmxITE9W5c2cFBgbq66+/1vHjx9WoUaMr6h9AMTl7EhKA61N0dLSRZFvKly9vWrdubVauXGlXFxcXZ/z9/Y2fn58ZOHCgefXVV03FihVt6//44w/Tt29fc8MNNxhJZuHChcYYY3799VcTExNj/P39jbe3t2natKn56KOPjDHnJ0H/eR/GGLNq1SpT1B9569evN3379jU1a9Y0np6eJigoyHTp0sVs3rzZVvPnSdAbNmywO74LS0hIiK3+xx9/NH369DE33HCD8fHxMQ0bNjSPPvqoKSgoMHv27DFRUVGmSpUqxsvLy9SvX9/MmjWr+CcawBVxMYZnLgEAgLUwCRoAAFgOAQgAAFgOAQgAAFgOAQgAAFgOAQgAAFgOAQgAAFgOAQgAAFgOAQgAAFgOAQgAAFgOAQgAAFgOAQgAAFgOAQgAAFjO/wOOeUpmOFNMyAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_error"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6VrQ-TcIxR5",
        "outputId": "808a8141-2097-4746-d7be-bf078fb0be0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[10.127924919128418,\n",
              " 10.127924919128418,\n",
              " 10.127924919128418,\n",
              " 10.127924919128418,\n",
              " 10.127924919128418,\n",
              " 10.127924919128418]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#best batch size cifar/softmax\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "x_train = x_train.reshape(50000, 32*32*3)\n",
        "x_test = x_test.reshape(10000, 32*32*3)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255.\n",
        "x_test /= 255.\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "def model_1(x_input):\n",
        "    x = Dense(512, activation='relu')(x_input)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x_out = Dense(256, activation='relu')(x)\n",
        "    x_out = Dropout(0.2)(x)\n",
        "    return x_out\n",
        "  \n",
        "def model_2(x_input):\n",
        "    x = Dense(800, activation='sigmoid')(x_input)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Dense(200, activation='sigmoid')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x_out = Dense(12)(x)\n",
        "    return x_out\n",
        "\n",
        "inputs = Input(shape=(32*32*3,))\n",
        "x      = model_1(inputs)\n",
        "x_out  = Dense(10, use_bias=False, activation='softmax')(x)\n",
        "model = Model(inputs, x_out)\n",
        "\n",
        "model.compile(optimizer='adam', loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
        "#optimizer = tf.train.AdamOptimizer(1.e-3)\n",
        "\n",
        "epochs = 4\n",
        "batch_size = [16, 32, 64, 128, 200, 256]\n",
        "best_batch_size = []\n",
        "test_error = []\n",
        "\n",
        "for i in batch_size:\n",
        "  x = model.fit(x_train, y_train,\n",
        "                      batch_size=i,\n",
        "                      epochs=epochs,\n",
        "                      verbose=1,\n",
        "                      validation_data=(x_test, y_test))\n",
        "  score = model.evaluate(x_test, y_test, verbose=0)\n",
        "  best_batch_size.append(score[1])\n",
        "  test_error.append(score[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYnZBVe7q47F",
        "outputId": "bb346424-9f4b-4324-f6da-435c56b7fe03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50000 train samples\n",
            "10000 test samples\n",
            "Epoch 1/4\n",
            "3125/3125 [==============================] - 81s 26ms/step - loss: 1.9383 - accuracy: 0.2846 - val_loss: 1.8040 - val_accuracy: 0.3406\n",
            "Epoch 2/4\n",
            "3125/3125 [==============================] - 76s 24ms/step - loss: 1.8153 - accuracy: 0.3354 - val_loss: 1.7833 - val_accuracy: 0.3527\n",
            "Epoch 3/4\n",
            "3125/3125 [==============================] - 75s 24ms/step - loss: 1.7861 - accuracy: 0.3481 - val_loss: 1.7370 - val_accuracy: 0.3664\n",
            "Epoch 4/4\n",
            "3125/3125 [==============================] - 74s 24ms/step - loss: 1.7669 - accuracy: 0.3581 - val_loss: 1.7146 - val_accuracy: 0.3814\n",
            "Epoch 1/4\n",
            "1563/1563 [==============================] - 43s 27ms/step - loss: 1.7143 - accuracy: 0.3810 - val_loss: 1.6854 - val_accuracy: 0.3904\n",
            "Epoch 2/4\n",
            "1563/1563 [==============================] - 44s 28ms/step - loss: 1.6934 - accuracy: 0.3884 - val_loss: 1.6674 - val_accuracy: 0.3967\n",
            "Epoch 3/4\n",
            "1563/1563 [==============================] - 44s 28ms/step - loss: 1.6799 - accuracy: 0.3951 - val_loss: 1.6697 - val_accuracy: 0.4044\n",
            "Epoch 4/4\n",
            "1563/1563 [==============================] - 43s 28ms/step - loss: 1.6663 - accuracy: 0.3998 - val_loss: 1.6564 - val_accuracy: 0.4026\n",
            "Epoch 1/4\n",
            "782/782 [==============================] - 33s 42ms/step - loss: 1.6213 - accuracy: 0.4174 - val_loss: 1.6063 - val_accuracy: 0.4244\n",
            "Epoch 2/4\n",
            "782/782 [==============================] - 26s 33ms/step - loss: 1.6140 - accuracy: 0.4229 - val_loss: 1.6134 - val_accuracy: 0.4252\n",
            "Epoch 3/4\n",
            "782/782 [==============================] - 27s 34ms/step - loss: 1.6095 - accuracy: 0.4227 - val_loss: 1.5926 - val_accuracy: 0.4300\n",
            "Epoch 4/4\n",
            "782/782 [==============================] - 27s 34ms/step - loss: 1.6008 - accuracy: 0.4248 - val_loss: 1.6526 - val_accuracy: 0.4055\n",
            "Epoch 1/4\n",
            "391/391 [==============================] - 17s 45ms/step - loss: 1.5685 - accuracy: 0.4388 - val_loss: 1.5636 - val_accuracy: 0.4445\n",
            "Epoch 2/4\n",
            "391/391 [==============================] - 18s 45ms/step - loss: 1.5605 - accuracy: 0.4396 - val_loss: 1.5848 - val_accuracy: 0.4318\n",
            "Epoch 3/4\n",
            "391/391 [==============================] - 17s 44ms/step - loss: 1.5541 - accuracy: 0.4418 - val_loss: 1.5526 - val_accuracy: 0.4467\n",
            "Epoch 4/4\n",
            "391/391 [==============================] - 17s 44ms/step - loss: 1.5519 - accuracy: 0.4447 - val_loss: 1.5675 - val_accuracy: 0.4476\n",
            "Epoch 1/4\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 1.5322 - accuracy: 0.4536 - val_loss: 1.5402 - val_accuracy: 0.4532\n",
            "Epoch 2/4\n",
            "250/250 [==============================] - 15s 58ms/step - loss: 1.5273 - accuracy: 0.4541 - val_loss: 1.5388 - val_accuracy: 0.4505\n",
            "Epoch 3/4\n",
            "250/250 [==============================] - 15s 58ms/step - loss: 1.5251 - accuracy: 0.4549 - val_loss: 1.5356 - val_accuracy: 0.4566\n",
            "Epoch 4/4\n",
            "250/250 [==============================] - 15s 58ms/step - loss: 1.5189 - accuracy: 0.4565 - val_loss: 1.5337 - val_accuracy: 0.4544\n",
            "Epoch 1/4\n",
            "196/196 [==============================] - 13s 69ms/step - loss: 1.5071 - accuracy: 0.4620 - val_loss: 1.5240 - val_accuracy: 0.4554\n",
            "Epoch 2/4\n",
            "196/196 [==============================] - 15s 78ms/step - loss: 1.5063 - accuracy: 0.4604 - val_loss: 1.5295 - val_accuracy: 0.4535\n",
            "Epoch 3/4\n",
            "196/196 [==============================] - 13s 68ms/step - loss: 1.5049 - accuracy: 0.4618 - val_loss: 1.5371 - val_accuracy: 0.4489\n",
            "Epoch 4/4\n",
            "196/196 [==============================] - 13s 68ms/step - loss: 1.4977 - accuracy: 0.4637 - val_loss: 1.5270 - val_accuracy: 0.4526\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = ['16', '32', '64', '128', '200', '256']\n",
        "\n",
        "plt.bar(batch_size, best_batch_size)\n",
        "plt.title('Best Batch Size (Cifar/Softmax)')\n",
        "plt.xlabel('Batch Sizes')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GTejfFFXA0Ak",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "d508feb6-d911-4569-d6a9-e8f79a459618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_error"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcxTIhGoREwH",
        "outputId": "3133f10b-74e0-431f-ba4a-6a361566c5a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.7145668268203735,\n",
              " 1.6563949584960938,\n",
              " 1.652593731880188,\n",
              " 1.5674803256988525,\n",
              " 1.533727765083313,\n",
              " 1.5270346403121948]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hAhtsQSYgE7"
      },
      "source": [
        "## EXTRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kyCJqCj3EBrv",
        "outputId": "58086fc3-7115-4686-934a-6f9814b758ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "60000 train samples\n",
            "10000 test samples\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_2 (Conv2D)           (None, 28, 28, 32)        320       \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 28, 28, 32)        0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 26, 26, 64)        18496     \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 26, 26, 64)        0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 13, 13, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 13, 13, 64)        0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 10816)             0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 250)               2704250   \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 250)               0         \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 250)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                2510      \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,725,576\n",
            "Trainable params: 2,725,576\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/4\n",
            "497/938 [==============>...............] - ETA: 1:56 - loss: 2.6511 - accuracy: 0.1012"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-94b49f5beee8>\u001b[0m in \u001b[0;36m<cell line: 59>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m               metrics=['accuracy'])\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#mnist softmax\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "img_rows, img_cols=28, 28\n",
        " \n",
        "if k.image_data_format() == 'channels_first':\n",
        "   x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "   x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "   inpx = (1, img_rows, img_cols)\n",
        " \n",
        "else:\n",
        "   x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "   x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "   inpx = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255.\n",
        "x_test /= 255.\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train)\n",
        "y_test = keras.utils.to_categorical(y_test)\n",
        "\n",
        "#define the convnet\n",
        "model = Sequential()\n",
        "# CONV => RELU => CONV => RELU => POOL => DROPOUT\n",
        "model.add(Conv2D(32, (3, 3), padding='same',input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('sigmoid'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "\n",
        "# FLATTERN => DENSE => RELU => DROPOUT\n",
        "model.add(Flatten())\n",
        "model.add(Dense(250))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# a softmax classifier\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "#opt = keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6)\n",
        "\n",
        "#opt = tf.keras.optimizers.legacy.RMSprop(lr=2e-3, decay=1e-5)\n",
        "#model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.Adadelta(),\n",
        "              loss=keras.losses.categorical_crossentropy,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train, y_train,batch_size=64,epochs=4,validation_data=(x_test, y_test),shuffle=True)\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EI03Jrbv9C-6",
        "outputId": "68b1a4bb-2c43-4a09-a93e-b31b7e435d2c"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 4s 0us/step\n",
            "x_train shape: (50000, 32, 32, 3)\n",
            "y_train shape: (50000, 1)\n",
            "50000 train samples\n",
            "10000 test samples\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 32, 32, 256)       7168      \n",
            "                                                                 \n",
            " activation (Activation)     (None, 32, 32, 256)       0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 30, 30, 256)       590080    \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 30, 30, 256)       0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 15, 15, 256)      0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 15, 15, 256)       0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 57600)             0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 512)               29491712  \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 512)               0         \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 10)                5130      \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 30,094,090\n",
            "Trainable params: 30,094,090\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/optimizers/legacy/rmsprop.py:143: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "782/782 [==============================] - 3695s 5s/step - loss: 3.1529 - accuracy: 0.2811 - val_loss: 1.7615 - val_accuracy: 0.3668\n",
            "Epoch 2/4\n",
            "782/782 [==============================] - 3646s 5s/step - loss: 1.5854 - accuracy: 0.4360 - val_loss: 1.5946 - val_accuracy: 0.4481\n",
            "Epoch 3/4\n",
            "530/782 [===================>..........] - ETA: 18:31 - loss: 1.4479 - accuracy: 0.4958"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import cifar10\n",
        "from keras.backend import relu\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix, classification_report\n",
        "import random\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "\n",
        "#LOADING DATASET\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "# Normalize the data. Before we need to connvert data type to float for computation.\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "# Convert class vectors to binary class matrices. This is called one hot encoding.\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "\n",
        "#define the convnet\n",
        "model = Sequential()\n",
        "# CONV => RELU => CONV => RELU => POOL => DROPOUT\n",
        "model.add(Conv2D(256, (3, 3), padding='same',input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(256, (3, 3)))\n",
        "model.add(Activation('sigmoid'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "\n",
        "# FLATTERN => DENSE => RELU => DROPOUT\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# a softmax classifier\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "#opt = keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6)\n",
        "opt = tf.keras.optimizers.legacy.RMSprop(lr=2e-3, decay=1e-5)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train, y_train,batch_size=64,epochs=4,validation_data=(x_test, y_test),shuffle=True)\n",
        "\n",
        "%matplotlib inline\n",
        "def plotmodelhistory(history): \n",
        "    fig, axs = plt.subplots(1,2,figsize=(15,5)) \n",
        "    # summarize history for accuracy\n",
        "    axs[0].plot(history.history['accuracy']) \n",
        "    axs[0].plot(history.history['val_accuracy']) \n",
        "    axs[0].set_title('Model Accuracy')\n",
        "    axs[0].set_ylabel('Accuracy') \n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].legend(['train', 'validate'], loc='upper left')\n",
        "    # summarize history for loss\n",
        "    axs[1].plot(history.history['loss']) \n",
        "    axs[1].plot(history.history['val_loss']) \n",
        "    axs[1].set_title('Model Loss')\n",
        "    axs[1].set_ylabel('Loss') \n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].legend(['train', 'validate'], loc='upper left')\n",
        "    plt.show()\n",
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "\n",
        "plotmodelhistory(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GRAPHS FOR DIFFERENT OPTIMIZERS"
      ],
      "metadata": {
        "id": "ljYYLM8HVqXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#MNIST (SVM)\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255.\n",
        "x_test /= 255.\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "def model_1(x_input):\n",
        "    x = Dense(512, activation='relu')(x_input)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x_out = Dense(256, activation='relu')(x)\n",
        "    x_out = Dropout(0.5)(x)\n",
        "    return x_out\n",
        "  \n",
        "def model_2(x_input):\n",
        "    x = Dense(800, activation='sigmoid')(x_input)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(200, activation='sigmoid')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x_out = Dense(12)(x)\n",
        "    return x_out\n",
        "\n",
        "inputs = Input(shape=(784,))\n",
        "x      = model_1(inputs)\n",
        "x_out  = Dense(10, use_bias=False, activation='linear', name='svm')(x)\n",
        "model = Model(inputs, x_out)\n",
        "\n",
        "def svm_loss(layer):\n",
        "    weights = layer.weights[0]\n",
        "    weights_tf = tf.convert_to_tensor(weights)\n",
        "    \n",
        "    def categorical_hinge_loss(y_true, y_pred):\n",
        "        pos = K.sum(y_true * y_pred, axis=-1)\n",
        "        neg = K.max((1.0 - y_true) * y_pred, axis=-1)\n",
        "        hinge_loss = K.mean(K.maximum(0.0, neg - pos + 1), axis=-1)\n",
        "        regularization_loss = 0.5*(tf.reduce_sum(tf.square(weights_tf)))\n",
        "        return regularization_loss + 0.4*hinge_loss\n",
        "    \n",
        "    return categorical_hinge_loss\n",
        "xLabels=['SGD','RMSProp','Adam','AdamW']\n",
        "yVal=[]\n",
        "metrics = ['accuracy']\n",
        "optimizer = tf.keras.optimizers.SGD(lr=0.1, momentum=0.9)\n",
        "\n",
        "#optimizer = tf.keras.optimizers.legacy.RMSprop(lr=2e-3, decay=1e-5)\n",
        "#optimizer = tf.train.AdamOptimizer(1.e-3)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=svm_loss(model.get_layer('svm')), metrics=metrics)\n",
        "\n",
        "batch_size = 200\n",
        "epochs = 4\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "yVal.append(score[1])\n",
        "\n",
        "#optimizer = tf.keras.optimizers.SGD(lr=0.1, momentum=0.9)\n",
        "\n",
        "optimizer = tf.keras.optimizers.legacy.RMSprop(lr=0.1, decay=1e-5, momentum=0.9)\n",
        "#optimizer = tf.train.AdamOptimizer(1.e-3)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=svm_loss(model.get_layer('svm')), metrics=metrics)\n",
        "\n",
        "batch_size = 200\n",
        "epochs = 4\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "yVal.append(score[1])\n",
        "\n",
        "#optimizer = tf.keras.optimizers.SGD(lr=0.1, momentum=0.9)\n",
        "\n",
        "#optimizer = tf.keras.optimizers.legacy.RMSprop(lr=2e-3, decay=1e-5)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=svm_loss(model.get_layer('svm')), metrics=metrics)\n",
        "\n",
        "batch_size = 200\n",
        "epochs = 4\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "yVal.append(score[1])\n",
        "\n",
        "#optimizer = tf.keras.optimizers.SGD(lr=0.1, momentum=0.9)\n",
        "\n",
        "#optimizer = tf.keras.optimizers.legacy.RMSprop(lr=2e-3, decay=1e-5)\n",
        "#optimizer = tf.train.AdamOptimizer(1.e-3)\n",
        "optimizer= tf.keras.optimizers.AdamW(learning_rate=0.1)\n",
        "model.compile(optimizer=optimizer, loss=svm_loss(model.get_layer('svm')), metrics=metrics)\n",
        "\n",
        "batch_size = 200\n",
        "epochs = 4\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "yVal.append(score[1])\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "import matplotlib.pyplot as plt\n",
        "plt.title('MNIST SVM Different Optimization Functions')\n",
        "plt.bar(xLabels, yVal)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NK7MjWb8VyKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MNIST (softmax)\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255.\n",
        "x_test /= 255.\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "def model_1(x_input):\n",
        "    x = Dense(512, activation='relu')(x_input)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x_out = Dense(256, activation='relu')(x)\n",
        "    x_out = Dropout(0.5)(x)\n",
        "    return x_out\n",
        "  \n",
        "def model_2(x_input):\n",
        "    x = Dense(800, activation='sigmoid')(x_input)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(200, activation='sigmoid')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x_out = Dense(12)(x)\n",
        "    return x_out\n",
        "xLabels=['SGD','RMSProp','Adam','AdamW']\n",
        "yVal=[]\n",
        "inputs = Input(shape=(784,))\n",
        "x      = model_1(inputs)\n",
        "x_out  = Dense(10, use_bias=False, activation='softmax')(x)\n",
        "model = Model(inputs, x_out)\n",
        "optimizer = tf.keras.optimizers.SGD(lr=0.1, momentum=0.9)\n",
        "\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=keras.losses.categorical_crossentropy,\n",
        "          \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#metrics = ['accuracy']\n",
        "#optimizer = tf.keras.optimizers.legacy.RMSprop(lr=2e-3, decay=1e-5)\n",
        "#optimizer = tf.train.AdamOptimizer(1.e-3)\n",
        "\n",
        "#model.compile(optimizer=optimizer, loss=svm_loss(model.get_layer('svm')), metrics=metrics)\n",
        "\n",
        "batch_size = 64\n",
        "epochs = 4\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "yVal.append(score[1])\n",
        "\n",
        "\n",
        "model = Model(inputs, x_out)\n",
        "optimizer = tf.keras.optimizers.legacy.RMSprop(lr=0.1, decay=1e-5, momentum=0.9)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=keras.losses.categorical_crossentropy,\n",
        "          \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#metrics = ['accuracy']\n",
        "#optimizer = tf.keras.optimizers.legacy.RMSprop(lr=2e-3, decay=1e-5)\n",
        "#optimizer = tf.train.AdamOptimizer(1.e-3)\n",
        "\n",
        "#model.compile(optimizer=optimizer, loss=svm_loss(model.get_layer('svm')), metrics=metrics)\n",
        "\n",
        "batch_size = 64\n",
        "epochs = 4\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "yVal.append(score[1])\n",
        "\n",
        "\n",
        "model = Model(inputs, x_out)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=keras.losses.categorical_crossentropy,\n",
        "          \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#metrics = ['accuracy']\n",
        "#optimizer = tf.keras.optimizers.legacy.RMSprop(lr=2e-3, decay=1e-5)\n",
        "#optimizer = tf.train.AdamOptimizer(1.e-3)\n",
        "\n",
        "#model.compile(optimizer=optimizer, loss=svm_loss(model.get_layer('svm')), metrics=metrics)\n",
        "\n",
        "batch_size = 64\n",
        "epochs = 4\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "yVal.append(score[1])\n",
        "\n",
        "\n",
        "model = Model(inputs, x_out)\n",
        "optimizer= tf.keras.optimizers.AdamW(learning_rate=0.1)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=keras.losses.categorical_crossentropy,\n",
        "          \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#metrics = ['accuracy']\n",
        "#optimizer = tf.keras.optimizers.legacy.RMSprop(lr=2e-3, decay=1e-5)\n",
        "#optimizer = tf.train.AdamOptimizer(1.e-3)\n",
        "\n",
        "#model.compile(optimizer=optimizer, loss=svm_loss(model.get_layer('svm')), metrics=metrics)\n",
        "\n",
        "batch_size = 64\n",
        "epochs = 4\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "yVal.append(score[1])\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.title('MNIST Softmax Different Optimization Functions')\n",
        "plt.bar(xLabels, yVal)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e6_QHwkkV1Bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CIFAR10 (SVM)\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "x_train = x_train.reshape(50000, 32*32*3)\n",
        "x_test = x_test.reshape(10000, 32*32*3)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255.\n",
        "x_test /= 255.\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "def model_1(x_input):\n",
        "    x = Dense(512, activation='relu')(x_input)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x_out = Dense(256, activation='relu')(x)\n",
        "    x_out = Dropout(0.2)(x)\n",
        "    return x_out\n",
        "  \n",
        "def model_2(x_input):\n",
        "    x = Dense(800, activation='sigmoid')(x_input)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Dense(200, activation='sigmoid')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x_out = Dense(12)(x)\n",
        "    return x_out\n",
        "\n",
        "inputs = Input(shape=(32*32*3,))\n",
        "x      = model_1(inputs)\n",
        "x_out  = Dense(10, use_bias=False, activation='linear', name='svm')(x)\n",
        "model = Model(inputs, x_out)\n",
        "\n",
        "def svm_loss(layer):\n",
        "    weights = layer.weights[0]\n",
        "    weights_tf = tf.convert_to_tensor(weights)\n",
        "    \n",
        "    def categorical_hinge_loss(y_true, y_pred):\n",
        "        pos = K.sum(y_true * y_pred, axis=-1)\n",
        "        neg = K.max((1.0 - y_true) * y_pred, axis=-1)\n",
        "        hinge_loss = K.mean(K.maximum(0.0, neg - pos + 1), axis=-1)\n",
        "        regularization_loss = 0.5*(tf.reduce_sum(tf.square(weights_tf)))\n",
        "        return regularization_loss + 0.4*hinge_loss\n",
        "    \n",
        "    return categorical_hinge_loss\n",
        "\n",
        "metrics = ['accuracy']\n",
        "#optimizer = tf.keras.optimizers.legacy.RMSprop(lr=2e-3, decay=1e-5)\n",
        "#optimizer = tf.train.AdamOptimizer(1.e-3)\n",
        "xLabels=['SGD','RMSProp','Adam','AdamW']\n",
        "yVal=[]\n",
        "optimizer = tf.keras.optimizers.SGD()\n",
        "model.compile(optimizer=optimizer, loss=svm_loss(model.get_layer('svm')), metrics=metrics)\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 4\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "yVal.append(score[1])\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "optimizer = tf.keras.optimizers.legacy.RMSprop()\n",
        "model.compile(optimizer=optimizer, loss=svm_loss(model.get_layer('svm')), metrics=metrics)\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 4\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "yVal.append(score[1])\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "model.compile(optimizer=optimizer, loss=svm_loss(model.get_layer('svm')), metrics=metrics)\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 4\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "yVal.append(score[1])\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "optimizer= tf.keras.optimizers.AdamW()\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=svm_loss(model.get_layer('svm')), metrics=metrics)\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 4\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "yVal.append(score[1])\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "import matplotlib.pyplot as plt\n",
        "plt.title('CIFAR10 SVM Different Optimization Functions')\n",
        "plt.bar(xLabels, yVal)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jlhC7Wd7V3Sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CIFAR10 (Softmax)\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "x_train = x_train.reshape(50000, 32*32*3)\n",
        "x_test = x_test.reshape(10000, 32*32*3)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255.\n",
        "x_test /= 255.\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "num_classes = 10\n",
        "xLabels=['SGD','RMSProp','Adam','AdamW']\n",
        "yVal=[]\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "def model_1(x_input):\n",
        "    x = Dense(512, activation='relu')(x_input)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x_out = Dense(256, activation='relu')(x)\n",
        "    x_out = Dropout(0.2)(x)\n",
        "    return x_out\n",
        "  \n",
        "def model_2(x_input):\n",
        "    x = Dense(800, activation='sigmoid')(x_input)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Dense(200, activation='sigmoid')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x_out = Dense(12)(x)\n",
        "    return x_out\n",
        "\n",
        "inputs = Input(shape=(32*32*3,))\n",
        "x      = model_1(inputs)\n",
        "x_out  = Dense(10, use_bias=False, activation='softmax')(x)\n",
        "model = Model(inputs, x_out)\n",
        "optimizer = tf.keras.optimizers.SGD()\n",
        "model.compile(optimizer=optimizer, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
        "#optimizer = tf.train.AdamOptimizer(1.e-3)\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 4\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "yVal.append(score[1])\n",
        "\n",
        "\n",
        "model = Model(inputs, x_out)\n",
        "optimizer = tf.keras.optimizers.legacy.RMSprop()\n",
        "model.compile(optimizer=optimizer, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
        "#optimizer = tf.train.AdamOptimizer(1.e-3)\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 4\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "yVal.append(score[1])\n",
        "\n",
        "\n",
        "model = Model(inputs, x_out)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "model.compile(optimizer=optimizer, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
        "#optimizer = tf.train.AdamOptimizer(1.e-3)\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 4\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "yVal.append(score[1])\n",
        "\n",
        "\n",
        "model = Model(inputs, x_out)\n",
        "optimizer= tf.keras.optimizers.AdamW()\n",
        "model.compile(optimizer=optimizer, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
        "#optimizer = tf.train.AdamOptimizer(1.e-3)\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 4\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "yVal.append(score[1])\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.title('CIFAR10 Softmax Different Optimization Functions')\n",
        "plt.bar(xLabels, yVal)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gW5e6LsBV5gV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}